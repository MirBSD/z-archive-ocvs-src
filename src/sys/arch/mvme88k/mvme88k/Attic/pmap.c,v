head	1.2;
access;
symbols
	cvs-200405160640:1.1.1.10
	cvs-200401271800:1.1.1.9
	cvs-200401261630:1.1.1.9
	cvs-200401021645:1.1.1.8
	cvs-200312222040:1.1.1.7
	cvs-200310020700:1.1.1.6
	cvs-200309271030:1.1.1.5
	cvs-200309261655:1.1.1.4
	cvs-200309251530:1.1.1.4
	cvs-200308302005:1.1.1.3
	cvs-200308171200:1.1.1.2
	ctm-3496:1.1.1.2
	ctm-3449:1.1.1.1
	ctm-3437:1.1.1.1
	cvs-200307191805:1.1.1.1
	ctm-3425:1.1.1.1
	cvs-200307091500:1.1.1.1
	cvs-200307072125:1.1.1.1
	ctm-3389:1.1.1.1
	cvs-200307021520:1.1.1.1
	cvs-200306291430:1.1.1.1
	ctm-3341:1.1.1.1
	cvs-200306082100:1.1.1.1
	ctm-3316:1.1.1.1
	ctm-3272:1.1.1.1
	ctm-3264:1.1.1.1
	cvs-200305071630:1.1.1.1
	ctm-3255:1.1.1.1
	ctm-3229:1.1.1.1
	ctm-3203:1.1.1.1
	cvs-20030410-1130:1.1.1.1
	ctm-3155:1.1.1.1
	ctm-3132:1.1.1.1
	openbsd:1.1.1;
locks; strict;
comment	@ * @;


1.2
date	2003.04.06.04.29.31;	author tg;	state dead;
branches;
next	1.1;

1.1
date	2003.03.22.17.50.06;	author tg;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2003.03.22.17.50.06;	author tg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2003.08.11.18.35.47;	author tg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2003.08.30.23.13.32;	author tg;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2003.09.25.16.35.58;	author tg;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2003.09.27.11.08.18;	author tg;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2003.10.02.07.39.40;	author tg;	state Exp;
branches;
next	1.1.1.7;

1.1.1.7
date	2003.12.22.21.00.01;	author tg;	state Exp;
branches;
next	1.1.1.8;

1.1.1.8
date	2004.01.02.17.49.46;	author tg;	state Exp;
branches;
next	1.1.1.9;

1.1.1.9
date	2004.01.26.18.42.37;	author tg;	state Exp;
branches;
next	1.1.1.10;

1.1.1.10
date	2004.05.16.08.37.43;	author tg;	state Exp;
branches;
next	;


desc
@@


1.2
log
@Remove "some dead architectures".

These are all but i386 and x86-64 (which is not in the tree yet),
because I have no machines to work with.

Re-adding support should be fairly easy, just cvs adding a partial
OpenBSD checkout and pulling in stuff like kernel signal patch, humantime,
etc. which got added to i386 as well.

You will need gcc support as well.
@
text
@/*	$OpenBSD: pmap.c,v 1.68 2003/01/24 09:57:44 miod Exp $	*/
/*
 * Copyright (c) 2001, 2002, 2003 Miodrag Vallat
 * Copyright (c) 1998-2001 Steve Murphree, Jr.
 * Copyright (c) 1996 Nivas Madhur
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Nivas Madhur.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */
/*
 * Mach Operating System
 * Copyright (c) 1991 Carnegie Mellon University
 * Copyright (c) 1991 OMRON Corporation
 * All Rights Reserved.
 *
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 */

#include <sys/types.h>
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/simplelock.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/msgbuf.h>
#include <sys/user.h>

#include <uvm/uvm.h>

#include <machine/asm_macro.h>
#include <machine/board.h>
#include <machine/cmmu.h>
#include <machine/cpu_number.h>
#include <machine/pmap_table.h>

/*
 *  VM externals
 */
extern vaddr_t      avail_start, avail_end;
extern vaddr_t      virtual_avail, virtual_end;

/*
 * Macros to operate pm_cpus field
 */
#define SETBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) |= (1 << (cpu_number));
#define CLRBIT_CPUSET(cpu_number, cpuset) (*(cpuset)) &= ~(1 << (cpu_number));

#ifdef	DEBUG
/*
 * Static variables, functions and variables for debugging
 */

/*
 * conditional debugging
 */
#define CD_NONE		0x00
#define CD_NORM		0x01
#define CD_FULL		0x02

#define CD_ACTIVATE	0x0000004	/* pmap_activate */
#define CD_KMAP		0x0000008	/* pmap_expand_kmap */
#define CD_MAP		0x0000010	/* pmap_map */
#define CD_CACHE	0x0000020	/* pmap_cache_ctrl */
#define CD_BOOT		0x0000040	/* pmap_bootstrap */
#define CD_INIT		0x0000080	/* pmap_init */
#define CD_CREAT	0x0000100	/* pmap_create */
#define CD_FREE		0x0000200	/* pmap_release */
#define CD_DESTR	0x0000400	/* pmap_destroy */
#define CD_RM		0x0000800	/* pmap_remove */
#define CD_RMAL		0x0001000	/* pmap_remove_all */
#define CD_PROT		0x0002000	/* pmap_protect */
#define CD_EXP		0x0004000	/* pmap_expand */
#define CD_ENT		0x0008000	/* pmap_enter */
#define CD_UPD		0x0010000	/* pmap_update */
#define CD_COL		0x0020000	/* pmap_collect */
#define CD_CBIT		0x0040000	/* pmap_changebit */
#define CD_TBIT		0x0080000	/* pmap_testbit */
#define CD_CREF		0x0100000	/* pmap_clear_reference */
#define CD_PGMV		0x0200000	/* pagemove */
#define CD_ALL		0x0FFFFFC

int pmap_con_dbg = CD_NONE;
#endif /* DEBUG */

struct pool pmappool, pvpool;

caddr_t vmmap;
pt_entry_t *vmpte, *msgbufmap;

struct pmap kernel_pmap_store;
pmap_t kernel_pmap = &kernel_pmap_store;

typedef struct kpdt_entry *kpdt_entry_t;
struct kpdt_entry {
	kpdt_entry_t	next;
	paddr_t		phys;
};
#define	KPDT_ENTRY_NULL		((kpdt_entry_t)0)

kpdt_entry_t	kpdt_free;

/*
 * MAX_KERNEL_VA_SIZE must fit into the virtual address space between
 * VM_MIN_KERNEL_ADDRESS and VM_MAX_KERNEL_ADDRESS.
 */

#define	MAX_KERNEL_VA_SIZE	(256*1024*1024)	/* 256 Mb */

/*
 * Size of kernel page tables, which is enough to map MAX_KERNEL_VA_SIZE
 */
#define	KERNEL_PDT_SIZE	(atop(MAX_KERNEL_VA_SIZE) * sizeof(pt_entry_t))

/*
 * Size of kernel page tables for mapping onboard IO space.
 */
#if defined(MVME188)
#define	M188_PDT_SIZE	(atop(UTIL_SIZE) * sizeof(pt_entry_t))
#else
#define	M188_PDT_SIZE 0
#endif

#if defined(MVME187) || defined(MVME197)
#define	M1x7_PDT_SIZE	(atop(OBIO_SIZE) * sizeof(pt_entry_t))
#else
#define	M1x7_PDT_SIZE 0
#endif

#if defined(MVME188) && defined(MVME187) || defined(MVME197)
#define	OBIO_PDT_SIZE	((brdtyp == BRD_188) ? M188_PDT_SIZE : M1x7_PDT_SIZE)
#else
#define	OBIO_PDT_SIZE	MAX(M188_PDT_SIZE, M1x7_PDT_SIZE)
#endif

#define MAX_KERNEL_PDT_SIZE	(KERNEL_PDT_SIZE + OBIO_PDT_SIZE)

/*
 * Two pages of scratch space.
 * Used in pmap_copy_page() and pmap_zero_page().
 */
vaddr_t phys_map_vaddr1, phys_map_vaddr2;

#define PV_ENTRY_NULL	((pv_entry_t) 0)

static pv_entry_t pg_to_pvh(struct vm_page *);

static __inline pv_entry_t
pg_to_pvh(struct vm_page *pg)
{
	return &pg->mdpage.pvent;
}

/*
 *	Locking primitives
 */

/*
 *	We raise the interrupt level to splvm, to block interprocessor
 *	interrupts during pmap operations.
 */
#define	SPLVM(spl)	spl = splvm();
#define	SPLX(spl)	splx(spl);

#define PMAP_LOCK(pmap,spl) \
	do { \
		SPLVM(spl); \
		simple_lock(&(pmap)->pm_lock); \
	} while (0)
#define PMAP_UNLOCK(pmap, spl) \
	do { \
		simple_unlock(&(pmap)->pm_lock); \
		SPLX(spl); \
	} while (0)

#define ETHERPAGES 16
void *etherbuf = NULL;
int etherlen;

#ifdef	PMAP_USE_BATC

/*
 * number of BATC entries used
 */
int batc_used;

/*
 * keep track BATC mapping
 */
batc_entry_t batc_entry[BATC_MAX];

#endif	/* PMAP_USE_BATC */

vaddr_t kmapva = 0;
extern vaddr_t bugromva;
extern vaddr_t sramva;
extern vaddr_t obiova;

/*
 * Internal routines
 */
void flush_atc_entry(long, vaddr_t, boolean_t);
pt_entry_t *pmap_expand_kmap(vaddr_t, vm_prot_t);
void pmap_remove_range(pmap_t, vaddr_t, vaddr_t);
void pmap_expand(pmap_t, vaddr_t);
void pmap_release(pmap_t);
vaddr_t pmap_map(vaddr_t, paddr_t, paddr_t, vm_prot_t, u_int);
pt_entry_t *pmap_pte(pmap_t, vaddr_t);
void pmap_remove_all(struct vm_page *);
void pmap_changebit(struct vm_page *, int, int);
boolean_t pmap_testbit(struct vm_page *, int);

/*
 * quick PTE field checking macros
 */
#define	pmap_pte_w(pte)		(*(pte) & PG_W)
#define	pmap_pte_m(pte)		(*(pte) & PG_M)
#define	pmap_pte_u(pte)		(*(pte) & PG_U)
#define	pmap_pte_prot(pte)	(*(pte) & PG_PROT)

#define	pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define	pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

/*
 * Convert machine-independent protection code to M88K protection bits.
 */
static __inline u_int32_t
m88k_protection(pmap_t pmap, vm_prot_t prot)
{
	pt_entry_t p;

	p = (prot & VM_PROT_WRITE) ? PG_RW : PG_RO;
	/*
	 * XXX this should not be necessary anymore now that pmap_enter
	 * does the correct thing... -- miod
	 */
#ifdef M88110
	if (cputyp == CPU_88110) {
		p |= PG_U;
		/* if the map is the kernel's map and since this
		 * is not a paged kernel, we go ahead and mark
		 * the page as modified to avoid an exception
		 * upon writing to the page the first time.  XXX smurph
		 */
		if (pmap == kernel_pmap) {
			if (p & PG_PROT)
				p |= PG_M;
		}
	}
#endif
	return p;
}

/*
 * Routine:	FLUSH_ATC_ENTRY
 *
 * Function:
 *	Flush atc(TLB) which maps given virtual address, in the CPUs which
 *	are specified by 'users', for the operating mode specified by
 *      'kernel'.
 *
 * Parameters:
 *	users	bit patterns of the CPUs which may hold the TLB, and
 *		should be flushed
 *	va	virtual address that should be flushed
 *      kernel  TRUE if supervisor mode, FALSE if user mode
 */
void
flush_atc_entry(long users, vaddr_t va, boolean_t kernel)
{
	int cpu;
	long tusers = users;

#ifdef DIAGNOSTIC
	if ((tusers != 0) && (ff1(tusers) >= MAX_CPUS)) {
		printf("ff1 users = %d!\n", ff1(tusers));
		panic("bogus amount of users!!!");
	}
#endif

	while ((cpu = ff1(tusers)) != 32) {
		if (cpu_sets[cpu]) { /* just checking to make sure */
			cmmu_flush_remote_tlb(cpu, kernel, va, PAGE_SIZE);
		}
		tusers &= ~(1 << cpu);
	}
}

/*
 * Routine:	PMAP_PTE
 *
 * Function:
 *	Given a map and a virtual address, compute a (virtual) pointer
 *	to the page table entry (PTE) which maps the address .
 *	If the page table associated with the address does not
 *	exist, PT_ENTRY_NULL is returned (and the map may need to grow).
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	virt	virtual address for which page table entry is desired
 *
 *    Otherwise the page table address is extracted from the segment table,
 *    the page table index is added, and the result is returned.
 */
pt_entry_t *
pmap_pte(pmap_t pmap, vaddr_t virt)
{
	sdt_entry_t *sdt;

#ifdef DIAGNOSTIC
	/*XXX will this change if physical memory is not contiguous? */
	/* take a look at PDTIDX XXXnivas */
	if (pmap == PMAP_NULL)
		panic("pmap_pte: pmap is NULL");
#endif

	sdt = SDTENT(pmap,virt);
	/*
	 * Check whether page table is exist or not.
	 */
	if (!SDT_VALID(sdt))
		return (PT_ENTRY_NULL);

	return (pt_entry_t *)(PG_PFNUM(*(sdt + SDT_ENTRIES)) << PDT_SHIFT) +
		PDTIDX(virt);
}

/*
 * Routine:	PMAP_EXPAND_KMAP (internal)
 *
 * Function:
 *    Allocate a page descriptor table (pte_table) and validate associated
 * segment table entry, returning pointer to page table entry. This is
 * much like 'pmap_expand', except that table space is acquired
 * from an area set up by pmap_bootstrap, instead of through
 * uvm_km_zalloc. (Obviously, because uvm_km_zalloc uses the kernel map
 * for allocation - which we can't do when trying to expand the
 * kernel map!) Note that segment tables for the kernel map were
 * all allocated at pmap_bootstrap time, so we only need to worry
 * about the page table here.
 *
 * Parameters:
 *	virt	VA for which translation tables are needed
 *	prot	protection attributes for segment entries
 *
 * Extern/Global:
 *	kpdt_free	kernel page table free queue
 *
 * Calls:
 *	m88k_protection
 *
 * This routine simply dequeues a table from the kpdt_free list,
 * initializes all its entries (invalidates them), and sets the
 * corresponding segment table entry to point to it. If the kpdt_free
 * list is empty - we panic (no other places to get memory, sorry). (Such
 * a panic indicates that pmap_bootstrap is not allocating enough table
 * space for the kernel virtual address space).
 *
 */
pt_entry_t *
pmap_expand_kmap(vaddr_t virt, vm_prot_t prot)
{
	sdt_entry_t template, *sdt;
	kpdt_entry_t kpdt_ent;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_KMAP | CD_FULL)) == (CD_KMAP | CD_FULL))
		printf("(pmap_expand_kmap: %x) v %x\n", curproc,virt);
#endif

	template = m88k_protection(kernel_pmap, prot) | SG_V;

	/* segment table entry derivate from map and virt. */
	sdt = SDTENT(kernel_pmap, virt);
	if (SDT_VALID(sdt))
		panic("pmap_expand_kmap: segment table entry VALID");

	kpdt_ent = kpdt_free;
	if (kpdt_ent == KPDT_ENTRY_NULL)
		panic("pmap_expand_kmap: Ran out of kernel pte tables");

	kpdt_free = kpdt_free->next;
	/* physical table */
	*sdt = kpdt_ent->phys | template;
	/* virtual table */
	*(sdt + SDT_ENTRIES) = (vaddr_t)kpdt_ent | template;
	kpdt_ent->phys = (paddr_t)0;
	kpdt_ent->next = NULL;

	return (pt_entry_t *)(kpdt_ent) + PDTIDX(virt);
}

/*
 * Routine:	PMAP_MAP
 *
 * Function:
 *    Map memory at initialization. The physical addresses being
 * mapped are not managed and are never unmapped.
 *
 * Parameters:
 *	virt	virtual address of range to map
 *	start	physical address of range to map
 *	end	physical address of end of range
 *	prot	protection attributes
 *	cmode	cache control attributes
 *
 * Calls:
 *	pmap_pte
 *	pmap_expand_kmap
 *
 * Special Assumptions
 *	For now, VM is already on, only need to map the specified
 * memory. Used only by pmap_bootstrap() and vm_page_startup().
 *
 * For each page that needs mapping:
 *	pmap_pte is called to obtain the address of the page table
 *	table entry (PTE). If the page table does not exist,
 *	pmap_expand_kmap is called to allocate it. Finally, the page table
 *	entry is set to point to the physical page.
 *
 *	initialize template with paddr, prot, dt
 *	look for number of phys pages in range
 *	{
 *		pmap_pte(virt)	- expand if necessary
 *		stuff pte from template
 *		increment virt one page
 *		increment template paddr one page
 *	}
 *
 */
vaddr_t
pmap_map(vaddr_t virt, paddr_t start, paddr_t end, vm_prot_t prot, u_int cmode)
{
	u_int npages;
	u_int num_phys_pages;
	pt_entry_t template, *pte;
	paddr_t	 page;
#ifdef	PMAP_USE_BATC
	batc_template_t	batctmp;
	int i;
#endif

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM))
		printf ("(pmap_map: %x) phys address from %x to %x mapped at virtual %x, prot %x cmode %x\n",
			curproc, start, end, virt, prot, cmode);
#endif

#ifdef DIAGNOSTIC
	if (start > end)
		panic("pmap_map: start greater than end address");
#endif

	template = m88k_protection(kernel_pmap, prot) | cmode | PG_V;

#ifdef	PMAP_USE_BATC
	batctmp.bits = 0;
	batctmp.field.sup = 1;	     /* supervisor */
	if (template & CACHE_WT)
		batctmp.field.wt = 1;	 /* write through */
	if (template & CACHE_GLOBAL)
		batctmp.field.g = 1;     /* global */
	if (template & CACHE_INH)
		batctmp.field.ci = 1;	 /* cache inhibit */
	if (template & PG_PROT)
		batctmp.field.wp = 1; /* protection */
	batctmp.field.v = 1;	     /* valid */
#endif

	page = trunc_page(start);
	npages = atop(round_page(end) - page);
	for (num_phys_pages = npages; num_phys_pages != 0; num_phys_pages--) {
#ifdef	PMAP_USE_BATC

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			printf("(pmap_map: %x) num_phys_pg=%x, virt=%x, "
			    "align V=%d, page=%x, align P=%d\n",
			    curproc, num_phys_pages, virt,
			    BATC_BLK_ALIGNED(virt), page,
			    BATC_BLK_ALIGNED(page));
#endif

		if (BATC_BLK_ALIGNED(virt) && BATC_BLK_ALIGNED(page) &&
		     num_phys_pages >= BATC_BLKBYTES/PAGE_SIZE &&
		     batc_used < BATC_MAX ) {
			/*
			 * map by BATC
			 */
			batctmp.field.lba = M88K_BTOBLK(virt);
			batctmp.field.pba = M88K_BTOBLK(page);

			for (i = 0; i < MAX_CPUS; i++)
				if (cpu_sets[i])
					cmmu_set_pair_batc_entry(i, batc_used,
								 batctmp.bits);
			batc_entry[batc_used] = batctmp.field;
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_MAP | CD_NORM)) == (CD_MAP | CD_NORM)) {
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp.bits);
			}
			if (pmap_con_dbg & CD_MAP)
				for (i = 0; i < BATC_BLKBYTES; i += PAGE_SIZE) {
					pte = pmap_pte(kernel_pmap, virt + i);
					if (PDT_VALID(pte))
						printf("(pmap_map: %x) va %x is already mapped: pte %x\n",
						    curproc, virt + i, *pte);
				}
#endif
			batc_used++;
			virt += BATC_BLKBYTES;
			page += BATC_BLKBYTES;
			num_phys_pages -= BATC_BLKBYTES/PAGE_SIZE;
			continue;
		}
#endif	/* PMAP_USE_BATC */
	
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pte = pmap_expand_kmap(virt,
			    VM_PROT_READ | VM_PROT_WRITE);

#ifdef DEBUG
		if ((pmap_con_dbg & (CD_MAP | CD_FULL)) == (CD_MAP | CD_FULL))
			if (PDT_VALID(pte))
				printf("(pmap_map: %x) pte @@ 0x%p already valid\n", curproc, pte);
#endif

		*pte = template | page;
		virt += PAGE_SIZE;
		page += PAGE_SIZE;
	}
	return virt;
}

/*
 * Routine:	PMAP_CACHE_CONTROL
 *
 * Function:
 *	Set the cache-control bits in the page table entries(PTE) which maps
 *	the specified virtual address range.
 *
 * Parameters:
 *	pmap_t		pmap
 *	vaddr_t		s
 *	vaddr_t		e
 *	u_int		mode
 *
 * Calls:
 *	pmap_pte
 *	invalidate_pte
 *	flush_atc_entry
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existant, nothing is done.
 * Otherwise, the cache-control bits in the PTE's are adjusted as specified.
 *
 */
void
pmap_cache_ctrl(pmap_t pmap, vaddr_t s, vaddr_t e, u_int mode)
{
	int spl;
	pt_entry_t *pte;
	vaddr_t va, pteva;
	boolean_t kflush;
	int cpu;
	u_int users;

#ifdef DEBUG
	if (mode & CACHE_MASK) {
		printf("(cache_ctrl) illegal mode %x\n",mode);
		return;
	}
	if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
		printf("(pmap_cache_ctrl: %x) pmap %x, va %x, mode %x\n", curproc, pmap, s, mode);
	}
#endif /* DEBUG */

#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
		panic("pmap_cache_ctrl: pmap is NULL");
#endif

	PMAP_LOCK(pmap, spl);

	users = pmap->pm_cpus;
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}

	for (va = s; va < e; va += PAGE_SIZE) {
		if ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL)
			continue;
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CACHE | CD_NORM)) == (CD_CACHE | CD_NORM)) {
			printf("(cache_ctrl) pte@@0x%p\n", pte);
		}
#endif /* DEBUG */
		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 * XXX
		 */
		*pte = (invalidate_pte(pte) & CACHE_MASK) | mode;
		flush_atc_entry(users, va, kflush);

		/*
		 * Data cache should be copied back and invalidated.
		 */
		pteva = ptoa(PG_PFNUM(*pte));
		for (cpu = 0; cpu < MAX_CPUS; cpu++)
			if (cpu_sets[cpu])
				cmmu_flush_remote_cache(cpu, pteva, PAGE_SIZE);
	}
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_BOOTSTRAP
 *
 * Function:
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, allocate the kernel
 *	translation table space, and map control registers
 *	and other IO addresses.
 *
 * Parameters:
 *	load_start	PA where kernel was loaded
 *	&phys_start	PA of first available physical page
 *	&phys_end	PA of last available physical page
 *	&virtual_avail	VA of first available page (after kernel bss)
 *	&virtual_end	VA of last available page (end of kernel address space)
 *
 * Extern/Global:
 *
 *	PAGE_SIZE	VM (software) page size
 *	kernelstart	start symbol of kernel text
 *	etext		end of kernel text
 *	phys_map_vaddr1 VA of page mapped arbitrarily for debug/IO
 *	phys_map_vaddr2 VA of page mapped arbitrarily for debug/IO
 *
 * Calls:
 *	simple_lock_init
 *	pmap_map
 *
 *    The physical address 'load_start' is mapped at
 * VM_MIN_KERNEL_ADDRESS, which maps the kernel code and data at the
 * virtual address for which it was (presumably) linked. Immediately
 * following the end of the kernel code/data, sufficient page of
 * physical memory are reserved to hold translation tables for the kernel
 * address space. The 'phys_start' parameter is adjusted upward to
 * reflect this allocation. This space is mapped in virtual memory
 * immediately following the kernel code/data map.
 *
 *    A pair of virtual pages are reserved for debugging and IO
 * purposes. They are arbitrarily mapped when needed. They are used,
 * for example, by pmap_copy_page and pmap_zero_page.
 *
 * For m88k, we have to map BUG memory also. This is a read only
 * mapping for 0x10000 bytes. We will end up having load_start as
 * 0 and VM_MIN_KERNEL_ADDRESS as 0 - yes sir, we have one-to-one
 * mapping!!!
 */

void
pmap_bootstrap(vaddr_t load_start, paddr_t *phys_start, paddr_t *phys_end,
    vaddr_t *virt_start, vaddr_t *virt_end)
{
	kpdt_entry_t kpdt_virt;
	sdt_entry_t *kmap;
	vaddr_t vaddr, virt, kernel_pmap_size, pdt_size;
	paddr_t s_text, e_text, kpdt_phys;
	apr_template_t apr_data;
	pt_entry_t *pte;
	int i;
	pmap_table_t ptable;
	extern void *kernelstart, *etext;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_NORM)) == (CD_BOOT | CD_NORM)) {
		printf("pmap_bootstrap: \"load_start\" 0x%x\n", load_start);
	}
#endif
#ifdef DIAGNOSTIC
	if (!PAGE_ALIGNED(load_start))
		panic("pmap_bootstrap: \"load_start\" not on the m88k page boundary: 0x%x", load_start);
#endif

	simple_lock_init(&kernel_pmap->pm_lock);

	/*
	 * Allocate the kernel page table from the front of available
	 * physical memory, i.e. just after where the kernel image was loaded.
	 */
	/*
	 * The calling sequence is
	 *    ...
	 *  pmap_bootstrap(&kernelstart,...);
	 *  kernelstart is the first symbol in the load image.
	 *  We link the kernel such that &kernelstart == 0x10000 (size of
	 *							BUG ROM)
	 *  The expression (&kernelstart - load_start) will end up as
	 *	0, making *virt_start == *phys_start, giving a 1-to-1 map)
	 */

	*phys_start = round_page(*phys_start);
	*virt_start = *phys_start +
	    (trunc_page((vaddr_t)&kernelstart) - load_start);

	/*
	 * Initialize kernel_pmap structure
	 */
	kernel_pmap->pm_count = 1;
	kernel_pmap->pm_cpus = 0;
	kernel_pmap->pm_stpa = kmap = (sdt_entry_t *)(*phys_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)(*virt_start);
	kmapva = *virt_start;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kernel_pmap->pm_stpa = 0x%x\n",kernel_pmap->pm_stpa);
		printf("kernel_pmap->pm_stab = 0x%x\n",kernel_pmap->pm_stab);
	}
#endif

	/*
	 * Reserve space for segment table entries.
	 * One for the regular segment table and one for the shadow table
	 * The shadow table keeps track of the virtual address of page
	 * tables. This is used in virtual-to-physical address translation
	 * functions. Remember, MMU cares only for physical addresses of
	 * segment and page table addresses. For kernel page tables, we
	 * really don't need this virtual stuff (since the kernel will
	 * be mapped 1-to-1) but for user page tables, this is required.
	 * Just to be consistent, we will maintain the shadow table for
	 * kernel pmap also.
	 */
	kernel_pmap_size = 2 * SDT_SIZE;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("     kernel segment start = 0x%x\n", kernel_pmap->pm_stpa);
		printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
		printf("       kernel segment end = 0x%x\n", ((paddr_t)kernel_pmap->pm_stpa) + kernel_pmap_size);
	}
#endif
	/* init all segment descriptors to zero */
	bzero(kernel_pmap->pm_stab, kernel_pmap_size);

	*phys_start += kernel_pmap_size;
	*virt_start += kernel_pmap_size;
	
	/* make sure page tables are page aligned!! XXX smurph */
	*phys_start = round_page(*phys_start);
	*virt_start = round_page(*virt_start);
	
	/* save pointers to where page table entries start in physical memory */
	kpdt_phys = *phys_start;
	kpdt_virt = (kpdt_entry_t)*virt_start;
	
	/* might as well round up to a page - XXX smurph */
	pdt_size = round_page(MAX_KERNEL_PDT_SIZE);
	kernel_pmap_size += pdt_size;
	*phys_start += pdt_size;
	*virt_start += pdt_size;

	/* init all page descriptors to zero */
	bzero((void *)kpdt_phys, pdt_size);
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("--------------------------------------\n");
		printf("        kernel page start = 0x%x\n", kpdt_phys);
		printf("   kernel page table size = 0x%x\n", pdt_size);
		printf("          kernel page end = 0x%x\n", *phys_start);
	}
#endif

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("kpdt_phys = 0x%x\n",kpdt_phys);
		printf("kpdt_virt = 0x%x\n",kpdt_virt);
		printf("end of kpdt at (virt)0x%08x, (phys)0x%08x\n",
		       *virt_start,*phys_start);
	}
#endif
	/*
	 * init the kpdt queue
	 */
	kpdt_free = kpdt_virt;
	for (i = pdt_size / PDT_SIZE; i != 0; i--) {
		kpdt_virt->next = (kpdt_entry_t)((vaddr_t)kpdt_virt + PDT_SIZE);
		kpdt_virt->phys = kpdt_phys;
		kpdt_virt = kpdt_virt->next;
		kpdt_phys += PDT_SIZE;
	}
	kpdt_virt->next = KPDT_ENTRY_NULL; /* terminate the list */

	/*
	 * Map the kernel image into virtual space
	 */

	s_text = load_start;	     /* paddr of text */
	e_text = load_start +
	    ((vaddr_t)&etext - trunc_page((vaddr_t)&kernelstart));
	/* paddr of end of text section*/
	e_text = round_page(e_text);

	/* map the first 64k (BUG ROM) read only, cache inhibited (? XXX) */
	vaddr = pmap_map(0, 0, 0x10000, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_INH);

	/* map the kernel text read only */
	vaddr = pmap_map(trunc_page((vaddr_t)&kernelstart),
	    s_text, e_text, VM_PROT_WRITE | VM_PROT_READ,
	    CACHE_GLOBAL);  /* shouldn't it be RO? XXX*/

	vaddr = pmap_map(vaddr, e_text, (paddr_t)kmap,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_GLOBAL);

	/*
	 * Map system segment & page tables - should be cache inhibited?
	 * 88200 manual says that CI bit is driven on the Mbus while accessing
	 * the translation tree. I don't think we need to map it CACHE_INH
	 * here...
	 */
	if (kmapva != vaddr) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("(pmap_bootstrap) correcting vaddr\n");
		}
#endif
		while (vaddr < (*virt_start - kernel_pmap_size))
			vaddr = round_page(vaddr + 1);
	}
	vaddr = pmap_map(vaddr, (paddr_t)kmap, *phys_start,
	    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);

	if (vaddr != *virt_start) {
		/*
		 * This should never happen because we now round the PDT
		 * table size up to a page boundry in the quest to get
		 * mc88110 working. - XXX smurph
		 */
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("1: vaddr %x *virt_start 0x%x *phys_start 0x%x\n", vaddr,
			       *virt_start, *phys_start);
		}
#endif
		*virt_start = vaddr;
		*phys_start = round_page(*phys_start);
	}

#if defined (MVME187) || defined (MVME197)
	/*
	 *  Get ethernet buffer - need etherlen bytes physically contiguous.
	 *  1 to 1 mapped as well???. There is actually a bug in the macros
	 *  used by the 1x7 ethernet driver. Remove this when that is fixed.
	 *  XXX -nivas
	 */
	if (brdtyp == BRD_187 || brdtyp == BRD_197) {
		*phys_start = vaddr;
		etherlen = ETHERPAGES * NBPG;
		etherbuf = (void *)vaddr;

		vaddr = pmap_map(vaddr, *phys_start, *phys_start + etherlen,
		    VM_PROT_WRITE | VM_PROT_READ, CACHE_INH);

		*virt_start += etherlen;
		*phys_start += etherlen;

		if (vaddr != *virt_start) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("2: vaddr %x *virt_start %x *phys_start %x\n", vaddr,
				       *virt_start, *phys_start);
			}
#endif
			*virt_start = vaddr;
			*phys_start = round_page(*phys_start);
		}
	}

#endif /* defined (MVME187) || defined (MVME197) */

	*virt_start = round_page(*virt_start);
	*virt_end = VM_MAX_KERNEL_ADDRESS;

	/*
	 * Map a few more pages for phys routines and debugger.
	 */

	phys_map_vaddr1 = round_page(*virt_start);
	phys_map_vaddr2 = phys_map_vaddr1 + PAGE_SIZE * max_cpus;

	/*
	 * To make 1:1 mapping of virt:phys, throw away a few phys pages.
	 * XXX what is this? nivas
	 */

	*phys_start += 2 * PAGE_SIZE * max_cpus;
	*virt_start += 2 * PAGE_SIZE * max_cpus;

	/*
	 * Map all IO space 1-to-1. Ideally, I would like to not do this
	 * but have va for the given IO address dynamically allocated. But
	 * on the 88200, 2 of the BATCs are hardwired to map the IO space
	 * 1-to-1; I decided to map the rest of the IO space 1-to-1.
	 * And bug ROM & the SRAM need to be mapped 1-to-1 if we ever want to
	 * execute bug system calls after the MMU has been turned on.
	 * OBIO should be mapped cache inhibited.
	 */

	ptable = pmap_table_build(0);
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("pmap_bootstrap: -> pmap_table_build\n");
	}
#endif

	for (; ptable->size != (size_t)(-1); ptable++){
		if (ptable->size) {
			/*
			 * size-1, 'cause pmap_map rounds up to next pagenumber
			 */
			pmap_map(ptable->virt_start, ptable->phys_start,
			    ptable->phys_start + (ptable->size - 1),
			    ptable->prot, ptable->cacheability);
		}
	}

	/*
	 * Allocate all the submaps we need. Note that SYSMAP just allocates
	 * kernel virtual address with no physical backing memory. The idea
	 * is physical memory will be mapped at this va before using that va.
	 * This means that if different physical pages are going to be mapped
	 * at different times, we better do a tlb flush before using it -
	 * else we will be referencing the wrong page.
	 */

#define	SYSMAP(c, p, v, n)	\
({ \
	v = (c)virt; \
    	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
    		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
	virt += ((n) * PAGE_SIZE); \
})

	virt = *virt_start;

	SYSMAP(caddr_t, vmpte, vmmap, 1);
	*vmpte = PG_NV;

	SYSMAP(struct msgbuf *, msgbufmap, msgbufp, btoc(MSGBUFSIZE));

	*virt_start = virt;

	/*
	 * Set translation for UPAGES at UADDR. The idea is we want to
	 * have translations set up for UADDR. Later on, the ptes for
	 * for this address will be set so that kstack will refer
	 * to the u area. Make sure pmap knows about this virtual
	 * address by doing vm_findspace on kernel_map.
	 */

	for (i = 0, virt = UADDR; i < UPAGES; i++, virt += PAGE_SIZE) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
			printf("setting up mapping for Upage %d @@ %x\n", i, virt);
		}
#endif
		if ((pte = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL)
			pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE);
	}
	/*
	 * Switch to using new page tables
	 */

	apr_data.bits = 0;
	apr_data.field.st_base = atop(kernel_pmap->pm_stpa);
	apr_data.field.wt = 1;
	apr_data.field.g  = 1;
	apr_data.field.ci = 0;
	apr_data.field.te = 1; /* Translation enable */
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		show_apr(apr_data.bits);
	}
#endif
	/* Invalidate entire kernel TLB. */
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("invalidating tlb %x\n", apr_data.bits);
	}
#endif

	for (i = 0; i < MAX_CPUS; i++)
		if (cpu_sets[i]) {
			/* Invalidate entire kernel TLB. */
			cmmu_flush_remote_tlb(i, 1, 0, -1);
			/* still physical */
			/*
			 * Set valid bit to DT_INVALID so that the very first
			 * pmap_enter() on these won't barf in
			 * pmap_remove_range().
			 */
			pte = pmap_pte(kernel_pmap, phys_map_vaddr1);
			*pte = PG_NV;
			pte = pmap_pte(kernel_pmap, phys_map_vaddr2);
			*pte = PG_NV;
			/* Load supervisor pointer to segment table. */
			cmmu_remote_set_sapr(i, apr_data.bits);
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
				printf("Processor %d running virtual.\n", i);
			}
#endif
			SETBIT_CPUSET(i, &kernel_pmap->pm_cpus);
		}

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_BOOT | CD_FULL)) == (CD_BOOT | CD_FULL)) {
		printf("running virtual - avail_next 0x%x\n", *phys_start);
	}
#endif
}

/*
 * Routine:	PMAP_INIT
 *
 * Function:
 *	Initialize the pmap module. It is called by vm_init, to initialize
 *	any structures that the pmap system needs to map virtual memory.
 *
 * Parameters:
 *	phys_start	physical address of first available page
 *			(was last set by pmap_bootstrap)
 *	phys_end	physical address of last available page
 *
 * Extern/Globals
 *	pmap_phys_start
 *	pmap_phys_end
 *
 * Calls:
 *	pool_init
 *
 *   This routine does not really have much to do. It initializes
 * pools for pmap structures and pv_entry structures.
 */
void
pmap_init(void)
{
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_INIT | CD_NORM)) == (CD_INIT | CD_NORM))
		printf("pmap_init()\n");
#endif

	pool_init(&pmappool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pvpool, sizeof(pv_entry_t), 0, 0, 0, "pvpl", NULL);
} /* pmap_init() */

/*
 * Routine:	PMAP_ZERO_PAGE
 *
 * Function:
 *	Zeroes the specified page.
 *
 * Parameters:
 *	pg		page to zero
 *
 * Extern/Global:
 *	phys_map_vaddr1
 *
 * Calls:
 *	m88k_protection
 *
 * Special Assumptions:
 *	no locking required
 *
 *	This routine maps the physical pages at the 'phys_map' virtual
 * address set up in pmap_bootstrap. It flushes the TLB to make the new
 * mappings effective, and zeros all the bits.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	vaddr_t srcva;
	int spl;
	int cpu;
	pt_entry_t *srcpte;

	cpu = cpu_number();
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu * PAGE_SIZE));
	srcpte = pmap_pte(kernel_pmap, srcva);

	SPLVM(spl);
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
	*srcpte = trunc_page(pa) |
	    m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;
	SPLX(spl);

	bzero((void *)srcva, PAGE_SIZE);
	/* force the data out */
	cmmu_flush_remote_data_cache(cpu, pa, PAGE_SIZE);
}

/*
 * Routine:	PMAP_CREATE
 *
 * Function:
 *	Create and return a physical map. If the size specified for the
 *	map is zero, the map is an actual physical map, and may be referenced
 *	by the hardware. If the size specified is non-zero, the map will be
 *	used in software only, and is bounded by that size.
 *
 *  This routines allocates a pmap structure.
 */
struct pmap *
pmap_create(void)
{
	pmap_t pmap;
	sdt_entry_t *segdt;
	u_int s;
#ifdef	PMAP_USE_BATC
	int i;
#endif

	pmap = pool_get(&pmappool, PR_WAITOK);
	bzero(pmap, sizeof(*pmap));

	/*
	 * Allocate memory for *actual* segment table and *shadow* table.
	 */
	s = round_page(2 * SDT_SIZE);
#ifdef DEBUG
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
		printf("(pmap_create: %x) need %d pages for sdt\n",
		       curproc, atop(s));
	}
#endif

	segdt = (sdt_entry_t *)uvm_km_zalloc(kernel_map, s);
	if (segdt == NULL)
		panic("pmap_create: kmem_alloc failure");
	
	/*
	 * Initialize pointer to segment table both virtual and physical.
	 */
	pmap->pm_stab = segdt;
	if (pmap_extract(kernel_pmap, (vaddr_t)segdt,
	    (paddr_t *)&pmap->pm_stpa) == FALSE)
		panic("pmap_create: pmap_extract failed!");

	if (!PAGE_ALIGNED(pmap->pm_stpa))
		panic("pmap_create: sdt_table 0x%x not aligned on page boundary",
		    (int)pmap->pm_stpa);

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_CREAT | CD_NORM)) == (CD_CREAT | CD_NORM)) {
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x, pm_stpa=0x%x\n",
		       curproc, pmap, pmap->pm_stab, pmap->pm_stpa);
	}
#endif

#ifdef MVME188
	if (brdtyp == BRD_188) {
		/*
		 * memory for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap,
		    (vaddr_t)segdt, (vaddr_t)segdt+ (SDT_SIZE*2),
		    CACHE_INH);
	}
#endif
	/*
	 * Initialize SDT_ENTRIES.
	 */
	/*
	 * There is no need to clear segment table, since kmem_alloc would
	 * provides us clean pages.
	 */

	/*
	 * Initialize pmap structure.
	 */
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_cpus = 0;

#ifdef	PMAP_USE_BATC
	/* initialize block address translation cache */
	for (i = 0; i < BATC_MAX; i++) {
		pmap->pm_ibatc[i].bits = 0;
		pmap->pm_dbatc[i].bits = 0;
	}
#endif

	return pmap;
}

/*
 * Routine:	PMAP_RELEASE
 *
 *	Internal procedure used by pmap_destroy() to actualy deallocate
 *	the tables.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *	PT_FREE
 *
 * Special Assumptions:
 *	No locking is needed, since this is only called which the
 * 	pm_count field of the pmap structure goes to zero.
 *
 * This routine sequences of through the user address space, releasing
 * all translation table space back to the system using PT_FREE.
 * The loops are indexed by the virtual address space
 * ranges represented by the table group sizes(PDT_TABLE_GROUP_VA_SPACE).
 *
 */
void
pmap_release(pmap_t pmap)
{
	unsigned long sdt_va;	/* outer loop index */
	sdt_entry_t *sdttbl;	/* ptr to first entry in the segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	u_int i, j;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
		printf("(pmap_release: %x) pmap %x\n", curproc, pmap);
#endif

	sdttbl = pmap->pm_stab;    /* addr of segment table */
	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
	  is near 0xffffffff
	*/
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if (j < 1024)
		j++;

	/* Segment table Loop */
	for (; i < j; i++) {
		sdt_va = PDT_TABLE_GROUP_VA_SPACE*i;
		if ((gdttbl = pmap_pte(pmap, (vaddr_t)sdt_va)) != PT_ENTRY_NULL) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
				printf("(pmap_release: %x) free page table = 0x%x\n",
				       curproc, gdttbl);
#endif
			PT_FREE(gdttbl);
		}
	} /* Segment Loop */

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_FULL)) == (CD_FREE | CD_FULL))
		printf("(pmap_release: %x) free segment table = 0x%x\n",
		       curproc, sdttbl);
#endif
	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2*SDT_SIZE));

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_FREE | CD_NORM)) == (CD_FREE | CD_NORM))
		printf("(pmap_release: %x) pm_count = 0\n", curproc);
#endif
}

/*
 * Routine:	PMAP_DESTROY
 *
 * Function:
 *	Retire the given physical map from service. Should only be called
 *	if the map contains no valid mappings.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	pmap_release
 *	pool_put
 *
 * Special Assumptions:
 *	Map contains no valid mappings.
 *
 *  This routine decrements the reference count in the pmap
 * structure. If it goes to zero, pmap_release is called to release
 * the memory space to the system. Then, call pool_put to free the
 * pmap structure.
 */
void
pmap_destroy(pmap_t pmap)
{
	int count;

#ifdef DIAGNOSTIC
	if (pmap == kernel_pmap)
		panic("pmap_destroy: Attempt to destroy kernel pmap");
#endif

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		pool_put(&pmappool, pmap);
	}
}


/*
 * Routine:	PMAP_REFERENCE
 *
 * Function:
 *	Add a reference to the specified  pmap.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Under a pmap read lock, the pm_count field of the pmap structure
 * is incremented. The function then returns.
 */
void
pmap_reference(pmap_t pmap)
{

	simple_lock(&pmap->pm_lock);
	pmap->pm_count++;
	simple_unlock(&pmap->pm_lock);
}

/*
 * Routine:	PMAP_REMOVE_RANGE (internal)
 *
 * Function:
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pool_put
 *	invalidate_pte
 *	flush_atc_entry
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *   This routine sequences through the pages defined by the given
 * range. For each page, pmap_pte is called to obtain a (virtual)
 * pointer to the page table  entry (PTE) associated with the page's
 * virtual address. If the page table entry does not exist, or is invalid,
 * nothing need be done.
 *
 *  If the PTE is valid, the routine must invalidated the entry. The
 * 'modified' bit, if on, is referenced to the VM, and into the appropriate
 * entry in the PV list entry. Next, the function must find the PV
 * list entry associated with this pmap/va (if it doesn't exist - the function
 * panics). The PV list entry is unlinked from the list, and returned to
 * its zone.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	pt_entry_t *pte, opte;
	pv_entry_t prev, cur, pvl;
	struct vm_page *pg;
	paddr_t pa;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	/*
	 * pmap has been locked by the caller.
	 */
	users = pmap->pm_cpus;
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}

	/*
	 * Loop through the range in vm_page_size increments.
	 * Do not assume that either start or end fail on any
	 * kind of page boundary (though this may be true!?).
	 */

	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;

		sdt = SDTENT(pmap,va);

		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
			continue;
		}

		pte = pmap_pte(pmap, va);

		if (!PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

		/*
		 * Update statistics.
		 */
		pmap->pm_stats.resident_count--;
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;

		pa = ptoa(PG_PFNUM(*pte));
		pg = PHYS_TO_VM_PAGE(pa);

		if (pg != NULL) {
			/*
			 * Remove the mapping from the pvlist for
			 * this physical page.
			 */
			pvl = pg_to_pvh(pg);

#ifdef DIAGNOSTIC
			if (pvl->pv_pmap == PMAP_NULL)
				panic("pmap_remove_range: null pv_list");
#endif

			if (pvl->pv_va == va && pvl->pv_pmap == pmap) {
				/*
				 * Hander is the pv_entry. Copy the next one
				 * to hander and free the next one (we can't
				 * free the hander)
				 */
				cur = pvl->pv_next;
				if (cur != PV_ENTRY_NULL) {
					*pvl = *cur;
					pool_put(&pvpool, cur);
				} else {
					pvl->pv_pmap =  PMAP_NULL;
				}

			} else {

				for (cur = pvl; cur != PV_ENTRY_NULL;
				    cur = cur->pv_next) {
					if (cur->pv_va == va && cur->pv_pmap == pmap)
						break;
					prev = cur;
				}
				if (cur == PV_ENTRY_NULL) {
					printf("pmap_remove_range: looking for VA "
					       "0x%x (pa 0x%x) PV list at 0x%p\n", va, pa, pvl);
					panic("pmap_remove_range: mapping not in pv_list");
				}

				prev->pv_next = cur->pv_next;
				pool_put(&pvpool, cur);
			}
		} /* if (pg != NULL) */

		/*
		 * Reflect modify bits to pager and zero (invalidate,
		 * remove) the pte entry.
		 */

		/*
		 * Invalidate pte temporarily to avoid being written back
		 * the modified bit and/or the reference bit by any other cpu.
		 */
		opte = invalidate_pte(pte);
		flush_atc_entry(users, va, kflush);

		if (opte & PG_M) {
			if (pg != NULL) {
				/* keep track ourselves too */
				pvl->pv_flags |= PG_M;
			}
		}

	} /* for (va = s; ...) */
}

/*
 * Routine:	PMAP_REMOVE
 *
 * Function:
 *	Remove the given range of addresses from the specified map.
 *	It is assumed that start and end are properly rounded to the VM page
 *	size.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s
 *	e
 *
 * Special Assumptions:
 *	Assumes not all entries must be valid in specified range.
 *
 * Calls:
 *	pmap_remove_range
 *
 *  After taking pmap read lock, pmap_remove_range is called to do the
 * real work.
 */
void
pmap_remove(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	int spl;

	if (pmap == PMAP_NULL)
		return;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
		printf("(pmap_remove: %x) map %x s %x e %x\n", curproc, pmap, s, e);
#endif

#ifdef DIAGNOSTIC
	if (s >= e)
		panic("pmap_remove: start greater than end address");
#endif

	PMAP_LOCK(pmap, spl);
	pmap_remove_range(pmap, s, e);
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_REMOVE_ALL
 *
 * Function:
 *	Removes this physical page from all physical maps in which it
 *	resides. Reflects back modify bits to the pager.
 *
 * Parameters:
 *	pg		physical pages which is to
 *			be removed from all maps
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	simple_lock
 *	pmap_pte
 *	pool_put
 *
 *  If the page specified by the given address is not a managed page,
 * this routine simply returns. Otherwise, the PV list associated with
 * that page is traversed. For each pmap/va pair pmap_pte is called to
 * obtain a pointer to the page table entry (PTE) associated with the
 * va (the PTE must exist and be valid, otherwise the routine panics).
 * The hardware 'modified' bit in the PTE is examined. If it is on, the
 * corresponding bit in the PV list entry corresponding
 * to the physical page is set to 1.
 * Then, the PTE is invalidated, and the PV list entry is unlinked and
 * freed.
 *
 *  At the end of this function, the PV list for the specified page
 * will be null.
 */
void
pmap_remove_all(struct vm_page *pg)
{
	pt_entry_t *pte;
	pv_entry_t pvl;
	vaddr_t va;
	pmap_t pmap;
	int spl;
#ifdef DEBUG
	int dbgcnt = 0;
#endif

	if (pg == NULL) {
		/* not a managed page. */
#ifdef DEBUG
		if (pmap_con_dbg & CD_RMAL)
			printf("(pmap_remove_all: %x) vm page 0x%x not a managed page\n", curproc, pg);
#endif
		return;
	}

	SPLVM(spl);
	/*
	 * Walk down PV list, removing all mappings.
	 * We don't have to lock the pv list, since we have the entire pmap
	 * system.
	 */
remove_all_Retry:

	pvl = pg_to_pvh(pg);

	/*
	 * Loop for each entry on the pv list
	 */
	while ((pmap = pvl->pv_pmap) != PMAP_NULL) {
		va = pvl->pv_va;
		if (!simple_lock_try(&pmap->pm_lock))
			goto remove_all_Retry;

		pte = pmap_pte(pmap, va);

		/*
		 * Do a few consistency checks to make sure
		 * the PV list and the pmap are in synch.
		 */
#ifdef DIAGNOSTIC
		if (pte == PT_ENTRY_NULL) {
#ifdef DEBUG
			printf("(pmap_remove_all: %p) vm page %p pmap %x va %x dbgcnt %x\n",
			       curproc, pg, pmap, va, dbgcnt);
#endif
			panic("pmap_remove_all: pte NULL");
		}
#endif	/* DIAGNOSTIC */
		if (!PDT_VALID(pte)) {
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
		}
		if (pmap_pte_w(pte)) {
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    pg);
#endif
			pvl = pvl->pv_next;
			goto next;
		}

		pmap_remove_range(pmap, va, va + PAGE_SIZE);

#ifdef DEBUG
		dbgcnt++;
#endif
		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);
}

/*
 * Routine:	PMAP_PROTECT
 *
 * Function:
 *	Sets the physical protection on the specified range of this map
 *	as requested.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		start address of start of range
 *	e		end address of end of range
 *	prot		desired protection attributes
 *
 *	Calls:
 *		PMAP_LOCK, PMAP_UNLOCK
 *		CHECK_PAGE_ALIGN
 *		panic
 *		pmap_pte
 *		SDT_NEXT
 *		PDT_VALID
 *
 *  This routine sequences through the pages of the specified range.
 * For each, it calls pmap_pte to acquire a pointer to the page table
 * entry (PTE). If the PTE is invalid, or non-existant, nothing is done.
 * Otherwise, the PTE's protection attributes are adjusted as specified.
 */
void
pmap_protect(pmap_t pmap, vaddr_t s, vaddr_t e, vm_prot_t prot)
{
	int spl;
	pt_entry_t *pte, ap;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	if (pmap == PMAP_NULL || prot & VM_PROT_WRITE)
		return;
	if ((prot & VM_PROT_READ) == 0) {
		pmap_remove(pmap, s, e);
		return;
	}

#ifdef DIAGNOSTIC
	if (s > e)
		panic("pmap_protect: start grater than end address");
#endif

	ap = m88k_protection(pmap, prot) & PG_PROT;

	PMAP_LOCK(pmap, spl);

	users = pmap->pm_cpus;
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}

	CHECK_PAGE_ALIGN(s, "pmap_protect");

	/*
	 * Loop through the range in vm_page_size increment.
	 */
	for (va = s; va < e; va += PAGE_SIZE) {
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1 << SDT_SHIFT)) {
				/* no page table, skip to next seg entry */
				va += (1 << SDT_SHIFT) - PAGE_SIZE;
			} else {
				/* wrap around */
				break;
			}
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect: %x) no page table: skip to 0x%x\n", curproc, va + PAGE_SIZE);
#endif
			continue;
		}

		if (!PDT_VALID(pte)) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_PROT | CD_FULL)) == (CD_PROT | CD_FULL))
				printf("(pmap_protect: %x) pte invalid pte @@ 0x%x\n", curproc, pte);
#endif
			continue;	 /*  no page mapping */
		}

		/*
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
		 * written back by any other cpu.
		 */
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
		flush_atc_entry(users, va, kflush);
		pte++;
	}
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_EXPAND
 *
 * Function:
 *	Expands a pmap to be able to map the specified virtual address.
 *	New kernel virtual memory is allocated for a page table
 *
 *	Must be called with the pmap system and the pmap unlocked, since
 *	these must be unlocked to use vm_allocate or vm_deallocate (via
 *	uvm_km_zalloc). Thus it must be called in a unlock/lock loop
 *	that checks whether the map has been expanded enough. ( We won't loop
 *	forever, since page table aren't shrunk.)
 *
 * Parameters:
 *	pmap	point to pmap structure
 *	v	VA indicating which tables are needed
 *
 * Extern/Global:
 *	user_pt_map
 *	kernel_pmap
 *
 * Calls:
 *	pmap_pte
 *	uvm_km_free
 *	uvm_km_zalloc
 *	pmap_extract
 *
 * Special Assumptions
 *	no pmap locks held
 *      pmap != kernel_pmap
 *
 * 1:	This routine immediately allocates space for a page table.
 *
 * 2:	The page table entries (PTEs) are initialized (set invalid), and
 *	the corresponding segment table entry is set to point to the new
 *	page table.
 */
void
pmap_expand(pmap_t pmap, vaddr_t v)
{
	int i, spl;
	vaddr_t pdt_vaddr;
	paddr_t pdt_paddr;
	sdt_entry_t *sdt;
	pt_entry_t *pte;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_EXP | CD_NORM)) == (CD_EXP | CD_NORM))
		printf ("(pmap_expand: %x) map %x v %x\n", curproc, pmap, v);
#endif

	CHECK_PAGE_ALIGN(v, "pmap_expand");

	/* XXX */
	pdt_vaddr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
	if (pmap_extract(kernel_pmap, pdt_vaddr, &pdt_paddr) == FALSE)
		panic("pmap_expand: pmap_extract failed");

#ifdef MVME188
	if (brdtyp == BRD_188) {
		/*
		 * the pages for page tables should be CACHE DISABLED on MVME188
		 */
		pmap_cache_ctrl(kernel_pmap, pdt_vaddr, pdt_vaddr+PAGE_SIZE, CACHE_INH);
	}
#endif

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, v)) != PT_ENTRY_NULL) {
		/*
		 * Someone else caused us to expand
		 * during our vm_allocate.
		 */
		PMAP_UNLOCK(pmap, spl);
		/* XXX */
		uvm_km_free(kernel_map, pdt_vaddr, PAGE_SIZE);

#ifdef DEBUG
		if (pmap_con_dbg & CD_EXP)
			printf("(pmap_expand: %x) table has already been allocated\n", curproc);
#endif
		return;
	}
	/*
	 * Apply a mask to V to obtain the vaddr of the beginning of
	 * its containing page 'table group',i.e. the group of
	 * page  tables that fit eithin a single VM page.
	 * Using that, obtain the segment table pointer that references the
	 * first page table in the group, and initialize all the
	 * segment table descriptions for the page 'table group'.
	 */
	v &= ~((1 << (LOG2_PDT_TABLE_GROUP_SIZE + PDT_BITS + PG_BITS)) - 1);

	sdt = SDTENT(pmap,v);

	/*
	 * Init each of the segment entries to point the freshly allocated
	 * page tables.
	 */
	for (i = PDT_TABLE_GROUP_SIZE; i>0; i--) {
		*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
		*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;
		sdt++;
		pdt_paddr += PDT_SIZE;
		pdt_vaddr += PDT_SIZE;
	}
	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_ENTER
 *
 * Function:
 *	Insert the given physical page (p) at the specified virtual
 *	address (v) in the target phisical map with the protecton requested.
 *	If specified, the page will be wired down, meaning that the
 *	related pte can not be reclaimed.
 *
 * N.B.: This is the only routine which MAY NOT lazy-evaluation or lose
 *	information. That is, this routine must actually insert this page
 *	into the given map NOW.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	va	VA of page to be mapped
 *	pa	PA of page to be mapped
 *	prot	protection attributes for page
 *	wired	wired attribute for page
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	m88k_protection
 *	pmap_pte
 *	pmap_expand
 *	pmap_remove_range
 *	PT_FREE
 *
 *	This routine starts off by calling pmap_pte to obtain a (virtual)
 * pointer to the page table entry corresponding to given virtual
 * address. If the page table itself does not exist, pmap_expand is
 * called to allocate it.
 *
 *      If the page table entry (PTE) already maps the given physical page,
 * all that is needed is to set the protection and wired attributes as
 * given. TLB entries are flushed and pmap_enter returns.
 *
 *	If the page table entry (PTE) maps a different physical page than
 * that given, the old mapping is removed by a call to map_remove_range.
 * And execution of pmap_enter continues.
 *
 *	To map the new physical page, the routine first inserts a new
 * entry in the PV list exhibiting the given pmap and virtual address.
 * It then inserts the physical page address, protection attributes, and
 * wired attributes into the page table entry (PTE).
 *
 *
 *	get machine-dependent prot code
 *	get the pte for this page
 *	if necessary pmap expand(pmap,v)
 *	if (changing wired attribute or protection) {
 * 		flush entry from TLB
 *		update template
 *		for (ptes per vm page)
 *			stuff pte
 *	} else if (mapped at wrong addr)
 *		flush entry from TLB
 *		pmap_remove_range
 *	} else {
 *		enter mapping in pv_list
 *		setup template and stuff ptes
 *	}
 *
 */
int
pmap_enter(pmap_t pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	int spl;
	pt_entry_t *pte, ap, template;
	paddr_t old_pa;
	pv_entry_t pv_e, pvl;
	u_int users;
	boolean_t kflush;
	boolean_t wired = (flags & PMAP_WIRED) != 0;
	struct vm_page *pg;

	CHECK_PAGE_ALIGN(va, "pmap_entry - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - PA");

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
		if (pmap == kernel_pmap)
			printf("(pmap_enter: %x) pmap kernel va %x pa %x\n", curproc, va, pa);
		else
			printf("(pmap_enter: %x) pmap %x va %x pa %x\n", curproc, pmap, va, pa);
	}
#endif

	ap = m88k_protection(pmap, prot);

	/*
	 * Must allocate a new pvlist entry while we're unlocked;
	 * zalloc may cause pageout (which will lock the pmap system).
	 * If we determine we need a pvlist entry, we will unlock
	 * and allocate one. Then will retry, throwing away
	 * the allocated entry later (if we no longer need it).
	 */
	pv_e = PV_ENTRY_NULL;

	PMAP_LOCK(pmap, spl);
	users = pmap->pm_cpus;

Retry:
	/*
	 * Expand pmap to include this pte.
	 */
	while ((pte = pmap_pte(pmap, va)) == PT_ENTRY_NULL) {
		if (pmap == kernel_pmap) {
			pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
		} else {
			/*
			 * Must unlock to expand the pmap.
			 */
			PMAP_UNLOCK(pmap, spl);
			pmap_expand(pmap, va);
			PMAP_LOCK(pmap, spl);
		}
	}
	/*
	 *	Special case if the physical page is already mapped
	 *	at this address.
	 */
	old_pa = ptoa(PG_PFNUM(*pte));
	if (old_pa == pa) {
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

		/*
		 * May be changing its wired attributes or protection
		 */

		if (wired && !(pmap_pte_w(pte)))
			pmap->pm_stats.wired_count++;
		else if (!wired && pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;

		if ((unsigned long)pa >= MAXPHYSMEM)
			template = CACHE_INH | PG_V;
		else
			template = CACHE_GLOBAL | PG_V;
		if (wired)
			template |= PG_W;

		/*
		 * If there is a same mapping, we have nothing to do.
		 */
		if (!PDT_VALID(pte) || pmap_pte_w_chg(pte, template & PG_W) ||
		    (pmap_pte_prot_chg(pte, ap & PG_PROT))) {

			/*
			 * Invalidate pte temporarily to avoid being written
			 * back the modified bit and/or the reference bit by
			 * any other cpu.
			 */
			template |= (invalidate_pte(pte) & PG_M);
			*pte++ = template | ap | trunc_page(pa);
			flush_atc_entry(users, va, kflush);
		}

	} else { /* if ( pa == old_pa) */
		/*
		 * Remove old mapping from the PV list if necessary.
		 */

		pg = PHYS_TO_VM_PAGE(pa);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
			if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
				printf("vaddr1 0x%x vaddr2 0x%x va 0x%x pa 0x%x managed %x\n",
				       phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				       pg != NULL ? 1 : 0);
				printf("pte %x pfn %x valid %x\n",
				       pte, PG_PFNUM(*pte), PDT_VALID(pte));
			}
		}
#endif
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(users, va, TRUE);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}

		if (pg != NULL) {
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
				if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
					printf("va 0x%x and managed pa 0x%x\n", va, pa);
				}
			}
#endif
			/*
			 *	Enter the mappimg in the PV list for this
			 *	physical page.
			 */
			pvl = pg_to_pvh(pg);

			if (pvl->pv_pmap == PMAP_NULL) {
				/*
				 *	No mappings yet
				 */
				pvl->pv_va = va;
				pvl->pv_pmap = pmap;
				pvl->pv_next = PV_ENTRY_NULL;

			} else {
#ifdef DEBUG
				/*
				 * check that this mapping is not already there
				 */
				{
					pv_entry_t e = pvl;
					while (e != PV_ENTRY_NULL) {
						if (e->pv_pmap == pmap && e->pv_va == va)
							panic("pmap_enter: already in pv_list");
						e = e->pv_next;
					}
				}
#endif
				/*
				 * Add new pv_entry after header.
				 */
				if (pv_e == PV_ENTRY_NULL) {
					pv_e = pool_get(&pvpool, PR_NOWAIT);
					goto Retry;
				}
				pv_e->pv_va = va;
				pv_e->pv_pmap = pmap;
				pv_e->pv_next = pvl->pv_next;
				pvl->pv_next = pv_e;
				/*
				 * Remember that we used the pvlist entry.
				 */
				pv_e = PV_ENTRY_NULL;
			}
		}

		/*
		 * And count the mapping.
		 */
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;

		if ((unsigned long)pa >= MAXPHYSMEM)
			template = CACHE_INH | PG_V;
		else
			template = CACHE_GLOBAL | PG_V;
		if (wired)
			template |= PG_W;

		if (flags & VM_PROT_WRITE)
			template |= PG_U | PG_M;
		else if (flags & VM_PROT_ALL)
			template |= PG_U;

		*pte = template | ap | trunc_page(pa);

	} /* if (pa == old_pa) ... else */

	PMAP_UNLOCK(pmap, spl);

	if (pv_e != PV_ENTRY_NULL)
		pool_put(&pvpool, pv_e);

	return 0;
}

/*
 * Routine:	pmap_unwire
 *
 * Function:	Change the wiring attributes for a map/virtual-address pair.
 *
 * Parameters:
 *	pmap	pointer to pmap structure
 *	v	virtual address of page to be unwired
 *
 * Calls:
 *	pmap_pte
 *
 * Special Assumptions:
 *	The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap_t pmap, vaddr_t v)
{
	pt_entry_t *pte;
	int spl;

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
		panic("pmap_unwire: pte missing");

	if (pmap_pte_w(pte)) {
		/* unwired mapping */
		pmap->pm_stats.wired_count--;
		*pte &= ~PG_W;
	}

	PMAP_UNLOCK(pmap, spl);
}

/*
 * Routine:	PMAP_EXTRACT
 *
 * Function:
 *	Extract the physical page address associoated
 *	with the given map/virtual_address pair.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	va		virtual address
 *	pap		storage for result.
 *
 * Calls:
 *	PMAP_LOCK, PMAP_UNLOCK
 *	pmap_pte
 *
 * If BATC mapping is enabled and the specified pmap is kernel_pmap,
 * batc_entry is scanned to find out the mapping.
 *
 * Then the routine calls pmap_pte to get a (virtual) pointer to
 * the page table entry (PTE) associated with the given virtual
 * address. If the page table does not exist, or if the PTE is not valid,
 * then 0 address is returned. Otherwise, the physical page address from
 * the PTE is returned.
 */
boolean_t
pmap_extract(pmap_t pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t *pte;
	paddr_t pa;
	int spl;
	boolean_t rv = FALSE;

#ifdef	PMAP_USE_BATC
	int i;
#endif

#ifdef DIAGNOSTIC
	if (pmap == PMAP_NULL)
		panic("pmap_extract: pmap is NULL");
#endif

#ifdef	PMAP_USE_BATC
	/*
	 * check BATC first
	 */
	if (pmap == kernel_pmap && batc_used != 0)
		for (i = batc_used - 1; i != 0; i--)
			if (batc_entry[i].lba == M88K_BTOBLK(va)) {
				if (pap != NULL)
					*pap = (batc_entry[i].pba << BATC_BLKSHIFT) |
						(va & BATC_BLKMASK);
				return TRUE;
			}
#endif

	PMAP_LOCK(pmap, spl);

	if ((pte = pmap_pte(pmap, va)) != PT_ENTRY_NULL) {
		if (PDT_VALID(pte)) {
			rv = TRUE;
			if (pap != NULL) {
				pa = ptoa(PG_PFNUM(*pte));
				pa |= (va & PAGE_MASK); /* offset within page */
				*pap = pa;
			}
		}
	}

	PMAP_UNLOCK(pmap, spl);
	return rv;
}

/*
 * Routine:	PMAP_COLLECT
 *
 * Runction:
 *	Garbage collects the physical map system for pages which are
 *	no longer used. there may well be pages which are not
 *	referenced, but others may be collected as well.
 *	Called by the pageout daemon when pages are scarce.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *
 * Calls:
 *	PT_FREE
 *	pmap_pte
 *	pmap_remove_range
 *
 *	The intent of this routine is to release memory pages being used
 * by translation tables. They can be release only if they contain no
 * valid mappings, and their parent table entry has been invalidated.
 *
 *	The routine sequences through the entries user address space,
 * inspecting page-sized groups of page tables for wired entries. If
 * a full page of tables has no wired enties, any otherwise valid
 * entries are invalidated (via pmap_remove_range). Then, the segment
 * table entries corresponding to this group of page tables are
 * invalidated. Finally, PT_FREE is called to return the page to the
 * system.
 *
 *	If all entries in a segment table are invalidated, it too can
 * be returned to the system.
 *
 *	[Note: depending upon compilation options, tables may be in zones
 * or allocated through kmem_alloc. In the former case, the
 * module deals with a single table at a time.]
 */
void
pmap_collect( pmap_t pmap)
{
	vaddr_t sdt_va;		/* outer loop index */
	vaddr_t sdt_vt;		/* end of segment */
	sdt_entry_t *sdttbl;	/* ptr to first entry in seg table */
	sdt_entry_t *sdtp;	/* ptr to index into segment table */
	sdt_entry_t *sdt;	/* ptr to index into segment table */
	pt_entry_t *gdttbl;	/* ptr to first entry in a page table */
	pt_entry_t *gdttblend;	/* ptr to byte after last entry in
				   table group */
	pt_entry_t *gdtp;	/* ptr to index into a page table */
	boolean_t found_gdt_wired; /* flag indicating a wired page exists
				   in a page table's address range */
	int spl;
	u_int i, j;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
		printf ("(pmap_collect: %x) pmap %x\n", curproc, pmap);
#endif

	PMAP_LOCK(pmap, spl);

	sdttbl = pmap->pm_stab; /* addr of segment table */
	sdtp = sdttbl;

	/*
	  This contortion is here instead of the natural loop
	  because of integer overflow/wraparound if VM_MAX_ADDRESS
	  is near 0xffffffff
	*/
	i = VM_MIN_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	j = VM_MAX_ADDRESS / PDT_TABLE_GROUP_VA_SPACE;
	if (j < 1024)
		j++;

	/* Segment table loop */
	for (; i < j; i++, sdtp += PDT_TABLE_GROUP_SIZE) {
		sdt_va = VM_MIN_ADDRESS + PDT_TABLE_GROUP_VA_SPACE * i;

		gdttbl = pmap_pte(pmap, sdt_va);

		if (gdttbl == PT_ENTRY_NULL)
			continue; /* no maps in this range */

		gdttblend = gdttbl + (PDT_ENTRIES * PDT_TABLE_GROUP_SIZE);

		/* scan page maps for wired pages */
		found_gdt_wired = FALSE;
		for (gdtp = gdttbl; gdtp < gdttblend; gdtp++) {
			if (pmap_pte_w(gdtp)) {
				found_gdt_wired = TRUE;
				break;
			}
		}

		if (found_gdt_wired)
			continue; /* can't free this range */

		/* figure out end of range. Watch for wraparound */
		sdt_vt = sdt_va <= VM_MAX_ADDRESS - PDT_TABLE_GROUP_VA_SPACE ?
		    sdt_va + PDT_TABLE_GROUP_VA_SPACE : VM_MAX_ADDRESS;

		/* invalidate all maps in this range */
		pmap_remove_range(pmap, sdt_va, sdt_vt);

		/*
		 * we can safely deallocate the page map(s)
		 */
		for (sdt = sdtp; sdt < (sdtp + PDT_TABLE_GROUP_SIZE); sdt++) {
			*((sdt_entry_t *) sdt) = 0;
			*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = 0;
		}

		/*
		 * we have to unlock before freeing the table, since PT_FREE
		 * calls uvm_km_free or free, which will invoke another
		 * pmap routine
		 */
		PMAP_UNLOCK(pmap, spl);
		PT_FREE(gdttbl);
		PMAP_LOCK(pmap, spl);
	}

	PMAP_UNLOCK(pmap, spl);

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_COL | CD_NORM)) == (CD_COL | CD_NORM))
		printf("(pmap_collect: %x) done\n", curproc);
#endif
}

/*
 * Routine:	PMAP_ACTIVATE
 *
 * Function:
 * 	Binds the given physical map to the given
 *	processor, and returns a hardware map description.
 *	In a mono-processor implementation the cpu
 *	argument is ignored, and the PMAP_ACTIVATE macro
 *	simply sets the MMU root pointer element of the PCB
 *	to the physical address of the segment descriptor table.
 *
 * Parameters:
 * 	p	pointer to proc structure
 *
 * Notes:
 *	If the specified pmap is not kernel_pmap, this routine makes arp
 *	template and stores it into UAPR (user area pointer register) in the
 *	CMMUs connected to the specified CPU.
 *
 *	If kernel_pmap is specified, only flushes the TLBs mapping kernel
 *	virtual space, in the CMMUs connected to the specified CPU.
 *
 */
void
pmap_activate(struct proc *p)
{
	apr_template_t apr_data;
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();

#ifdef	PMAP_USE_BATC
	int n;
#endif

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ACTIVATE | CD_NORM)) == (CD_ACTIVATE | CD_NORM))
		printf("(pmap_activate: %x) pmap 0x%p\n", p, pmap);
#endif

	if (pmap != kernel_pmap) {
		/*
		 * Lock the pmap to put this cpu in its active set.
		 */

		simple_lock(&pmap->pm_lock);
		apr_data.bits = 0;
		apr_data.field.st_base = atop(pmap->pm_stpa);
		apr_data.field.wt = 0;
		apr_data.field.g  = 1;
		apr_data.field.ci = 0;
		apr_data.field.te = 1;

#ifdef	PMAP_USE_BATC
		/*
		 * cmmu_pmap_activate will set the uapr and the batc entries,
		 * then flush the *USER* TLB.  IF THE KERNEL WILL EVER CARE
		 * ABOUT THE BATC ENTRIES, THE SUPERVISOR TLBs SHOULB BE
		 * FLUSHED AS WELL.
		 */
		cmmu_pmap_activate(cpu, apr_data.bits,
				   pmap->pm_ibatc, pmap->pm_dbatc);
		for (n = 0; n < BATC_MAX; n++)
			*(register_t *)&batc_entry[n] = pmap->pm_ibatc[n].bits;
#else
		cmmu_set_uapr(apr_data.bits);
		cmmu_flush_tlb(FALSE, 0, -1);
#endif	/* PMAP_USE_BATC */

		/*
		 * Mark that this cpu is using the pmap.
		 */
		SETBIT_CPUSET(cpu, &(pmap->pm_cpus));

		simple_unlock(&pmap->pm_lock);
	}
}

/*
 * Routine:	PMAP_DEACTIVATE
 *
 * Function:
 *	Unbinds the given physical map from the given processor,
 *	i.e. the pmap i no longer is use on the processor.
 *
 * Parameters:
 *     	p		pointer to proc structure
 *
 * pmap_deactive simply clears the pm_cpus field in given pmap structure.
 *
 */
void
pmap_deactivate(struct proc *p)
{
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
	int cpu = cpu_number();
	
	if (pmap != kernel_pmap) {
		/*
		 * we expect the spl is already raised to sched level.
		 */
		simple_lock(&pmap->pm_lock);
		CLRBIT_CPUSET(cpu, &(pmap->pm_cpus));
		simple_unlock(&pmap->pm_lock);
	}
}

/*
 * Routine:	PMAP_COPY_PAGE
 *
 * Function:
 *	Copies the specified pages.
 *
 * Parameters:
 *	src	PA of source page
 *	dst	PA of destination page
 *
 * Extern/Global:
 *	phys_map_vaddr1
 *	phys_map_vaddr2
 *
 * Calls:
 *	m88k_protection
 *
 * Special Assumptions:
 *	no locking reauired
 *
 * This routine maps the physical pages at the 'phys_map' virtual
 * addresses set up in pmap_bootstrap. It flushes the TLB to make the
 * new mappings effective, and performs the copy.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
	vaddr_t dstva, srcva;
	int spl;
	pt_entry_t template, *dstpte, *srcpte;
	int cpu = cpu_number();

	template = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V;

	/*
	 * Map source physical address.
	 */
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	dstva = (vaddr_t)(phys_map_vaddr2 + (cpu << PAGE_SHIFT));

	srcpte = pmap_pte(kernel_pmap, srcva);
	dstpte = pmap_pte(kernel_pmap, dstva);

	SPLVM(spl);
	cmmu_flush_tlb(TRUE, srcva, PAGE_SIZE);
	*srcpte = template | trunc_page(src);

	/*
	 * Map destination physical address.
	 */
	cmmu_flush_tlb(TRUE, dstva, PAGE_SIZE);
	*dstpte  = template | trunc_page(dst);
	SPLX(spl);

	bcopy((void *)srcva, (void *)dstva, PAGE_SIZE);
	/* flush source, dest out of cache? */
	cmmu_flush_remote_data_cache(cpu, src, PAGE_SIZE);
	cmmu_flush_remote_data_cache(cpu, dst, PAGE_SIZE);
}

/*
 * Routine:	PMAP_CHANGEBIT
 *
 * Function:
 *	Update the pte bits on the specified physical page.
 *
 * Parameters:
 *	pg	physical page
 *	set	bits to set
 *	mask	bits to mask
 *
 * Extern/Global:
 *	pv_lists
 *
 * Calls:
 *	pmap_pte
 *
 * The pte bits corresponding to the page's frame index will be changed as
 * requested. The PV list will be traversed.
 * For each pmap/va the hardware the necessary bits in the page descriptor
 * table entry will be altered as well if necessary. If any bits were changed,
 * a TLB flush will be performed.
 */
void
pmap_changebit(struct vm_page *pg, int set, int mask)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, npte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	SPLVM(spl);

changebit_Retry:
	pvl = pg_to_pvh(pg);

	/*
	 * Clear saved attributes (modify, reference)
	 */
	pvl->pv_flags &= mask;

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_CBIT | CD_NORM)) == (CD_CBIT | CD_NORM))
			printf("(pmap_changebit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return;
	}

	/* for each listed pmap, update the affected bits */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		va = pvep->pv_va;
		if (!simple_lock_try(&pmap->pm_lock)) {
			goto changebit_Retry;
		}
		users = pmap->pm_cpus;
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

		pte = pmap_pte(pmap, va);

#ifdef DIAGNOSTIC
		/*
		 * Check for existing and valid pte
		 */
		if (pte == PT_ENTRY_NULL)
			panic("pmap_changebit: bad pv list entry.");
		if (!PDT_VALID(pte))
			panic("pmap_changebit: invalid pte");
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_changebit: pte doesn't point to page");
#endif

		/*
		 * Update bits
		 */
		*pte = invalidate_pte(pte);
		npte = (*pte | set) & mask;

		/*
		 * Flush TLB of which cpus using pmap.
		 *
		 * Invalidate pte temporarily to avoid the modified bit
		 * and/or the reference being written back by any other cpu.
		 */
		if (npte != *pte) {
			*pte = npte;
			flush_atc_entry(users, va, kflush);
		}

		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);
}

/*
 * Routine:	PMAP_CLEAR_MODIFY
 *
 * Function:
 *	Clears the modify bits on the specified physical page.
 */
boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	boolean_t rv;

	rv = pmap_testbit(pg, PG_M);
	pmap_changebit(pg, 0, ~PG_M);
	return rv;
}

/*
 * Routine:	PMAP_TESTBIT
 *
 * Function:
 *	Test the modified/referenced bits of a physical page.
 *
 * Parameters:
 *	pg	physical page
 *	bit	bit to test
 *
 * Extern/Global:
 *	pv lists
 *
 * Calls:
 *	simple_lock, simple_unlock
 *	pmap_pte
 *
 * If the attribute list for the given page has the bit, this routine
 * returns TRUE.
 *
 * Otherwise, this routine walks the PV list corresponding to the
 * given page. For each pmap/va pair, the page descripter table entry is
 * examined. If the selected bit is found on, the function returns TRUE
 * immediately (doesn't need to walk remainder of list), and updates the
 * attribute list.
 */
boolean_t
pmap_testbit(struct vm_page *pg, int bit)
{
	pv_entry_t pvl, pvep;
	pt_entry_t *pte;
	int spl;
	boolean_t rv;

	SPLVM(spl);

	pvl = pg_to_pvh(pg);
testbit_Retry:

	if (pvl->pv_flags & bit) {
		/* we've already cached a this flag for this page,
		   no use looking further... */
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit: %x) already cached a modify flag for this page\n",
			    curproc);
#endif
		SPLX(spl);
		return (TRUE);
	}

	if (pvl->pv_pmap == PMAP_NULL) {
		/* unmapped page - get info from attribute array
		   maintained by pmap_remove_range/pmap_remove_all */
		rv = (boolean_t)(pvl->pv_flags & bit);
#ifdef DEBUG
		if ((pmap_con_dbg & (CD_TBIT | CD_NORM)) == (CD_TBIT | CD_NORM))
			printf("(pmap_testbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return (rv);
	}

	/* for each listed pmap, check modified bit for given page */
	pvep = pvl;
	while (pvep != PV_ENTRY_NULL) {
		if (!simple_lock_try(&pvep->pv_pmap->pm_lock)) {
			goto testbit_Retry;
		}

		pte = pmap_pte(pvep->pv_pmap, pvep->pv_va);
		if (pte == PT_ENTRY_NULL) {
			printf("pmap_testbit: pte from pv_list not in map virt = 0x%x\n", pvep->pv_va);
			panic("pmap_testbit: bad pv list entry");
		}
		if (*pte & bit) {
			simple_unlock(&pvep->pv_pmap->pm_lock);
			pvl->pv_flags |= bit;
#ifdef DEBUG
			if ((pmap_con_dbg & (CD_TBIT | CD_FULL)) == (CD_TBIT | CD_FULL))
				printf("(pmap_testbit: %x) modified page pte@@0x%p\n", curproc, pte);
#endif
			SPLX(spl);
			return (TRUE);
		}
		simple_unlock(&pvep->pv_pmap->pm_lock);
		pvep = pvep->pv_next;
	}

	SPLX(spl);
	return (FALSE);
}

/*
 * Routine:	PMAP_IS_MODIFIED
 *
 * Function:
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */
boolean_t
pmap_is_modified(struct vm_page *pg)
{
	return pmap_testbit(pg, PG_M);
}

/*
 * Routine:	PMAP_CLEAR_REFERENCE
 *
 * Function:
 *	Clear the reference bit on the specified physical page.
 */
boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	boolean_t rv;

	rv = pmap_testbit(pg, PG_U);
	pmap_changebit(pg, 0, ~PG_U);
	return rv;
}

/*
 * Routine:	PMAP_IS_REFERENCED
 *
 * Function:
 *	Return whether or not the specified physical page is referenced by
 *	any physical maps.
 */
boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	return pmap_testbit(pg, PG_U);
}

/*
 * Routine:	PMAP_PAGE_PROTECT
 *
 * Calls:
 *	pmap_changebit
 *	pmap_remove_all
 *
 *	Lower the permission for all mappings to a given page.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	switch (prot) {
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		/* copy on write */
		pmap_changebit(pg, PG_RO, ~0);
		break;
	case VM_PROT_READ|VM_PROT_WRITE:
	case VM_PROT_ALL:
		break;
	default:
		pmap_remove_all(pg);
		break;
	}
}

void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = virtual_avail;
	*endp = virtual_end;
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	int spl;
	pt_entry_t template, *pte;
	u_int users;

	CHECK_PAGE_ALIGN(va, "pmap_kenter_pa - VA");
	CHECK_PAGE_ALIGN(pa, "pmap_kenter_pa - PA");

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_ENT | CD_NORM)) == (CD_ENT | CD_NORM)) {
		printf ("(pmap_kenter_pa: %x) va %x pa %x\n", curproc, va, pa);
	}
#endif

	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->pm_cpus;

	template = m88k_protection(kernel_pmap, prot);

	/*
	 * Expand pmap to include this pte.
	 */
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL) {
		pmap_expand_kmap(va, VM_PROT_READ|VM_PROT_WRITE);
	}

	/*
	 * And count the mapping.
	 */
	kernel_pmap->pm_stats.resident_count++;
	kernel_pmap->pm_stats.wired_count++;

	invalidate_pte(pte);
	if ((unsigned long)pa >= MAXPHYSMEM)
		template |= CACHE_INH | PG_V | PG_W;
	else
		template |= CACHE_GLOBAL | PG_V | PG_W;
	*pte = template | trunc_page(pa);
	flush_atc_entry(users, va, TRUE);

	PMAP_UNLOCK(kernel_pmap, spl);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	int spl;
	u_int users;

#ifdef DEBUG
	if ((pmap_con_dbg & (CD_RM | CD_NORM)) == (CD_RM | CD_NORM))
		printf("(pmap_kremove: %x) va %x len %x\n", curproc, va, len);
#endif

	CHECK_PAGE_ALIGN(va, "pmap_kremove addr");
	CHECK_PAGE_ALIGN(len, "pmap_kremove len");

	PMAP_LOCK(kernel_pmap, spl);
	users = kernel_pmap->pm_cpus;

	for (len >>= PAGE_SHIFT; len != 0; len--, va += PAGE_SIZE) {
		vaddr_t e = va + PAGE_SIZE;
		sdt_entry_t *sdt;
		pt_entry_t *pte;

		sdt = SDTENT(kernel_pmap, va);

		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;	/* align to segment */
			if (va <= e - (1<<SDT_SHIFT))
				va += (1<<SDT_SHIFT) - PAGE_SIZE; /* no page table, skip to next seg entry */
			else /* wrap around */
				break;
			continue;
		}

		/*
		 * Update the counts
		 */
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;

		pte = pmap_pte(kernel_pmap, va);
		invalidate_pte(pte);
		flush_atc_entry(users, va, TRUE);
	}
	PMAP_UNLOCK(map, spl);
}
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import OpenBSD 3.3 source repository from CTM 3132 the first time
This opens an OpenBSD-mirabile (aka MirBSD) repository.

### MirBSD is:
# Copyright (c) 1982-2003 by Thorsten "mirabile" Glaser <x86@@ePost.de>
# Copyright  1968-2003  The authors of And contributors to UNIX, the
#       C Language, BSD/Berkeley Unix; 386BSD, NetBSD 1.1 and OpenBSD.
#
# Anyone who obtained a copy of this work is hereby permitted to freely use,
# distribute, modify, merge, sublicence, give away or sell it as long as the
# authors are given due credit and the following notice is retained:
#
# This work is provided "as is", with no explicit or implicit warranty what-
# soever. Use it only at your own risk. In no event may an author or contri-
# butor be held liable for any damage, directly or indirectly, that origina-
# ted through or is caused by creation or modification of this work.

MirBSD is my private tree. MirBSD does not differ very much from OpenBSD
and intentionally tracks OpenBSD. That's why it _is_ OpenBSD, just not the
official one. It's like with DarrenBSD.

At time of this writing, no advertising for MirBSD must be done,
because the advertising clause has not yet been sorted out.

http://templeofhate.com/tglaser/MirBSD/index.php
@
text
@@


1.1.1.2
log
@Import the complete OpenBSD source tree (base system)
as of CTM delta 3496 (roughly 1200 UTC today) into the
vendor branch.
Attention: this is a big update. Don't even try to
build this system, OpenBSD 3.4-beta, yet on your own.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2003/08/08 21:36:33 miod Exp $	*/
d159 1
a159 1
#if defined(MVME188) && (defined(MVME187) || defined(MVME197))
d191 2
a192 2
#define	SPLVM(spl)	spl = splvm()
#define	SPLX(spl)	splx(spl)
d231 1
a296 2
#if NCPUS > 1
void flush_atc_entry(long, vaddr_t, boolean_t);
a316 4
#else
#define	flush_atc_entry(users,va,kernel) \
	cmmu_flush_remote_tlb(0, (kernel), (va), PAGE_SIZE)
#endif
d1285 1
a1285 1
		sdt_va = PDT_TABLE_GROUP_VA_SPACE * i;
d1649 2
a1650 1
	while (pvl != PV_ENTRY_NULL && (pmap = pvl->pv_pmap) != PMAP_NULL) {
a1653 1
		va = pvl->pv_va;
d1733 2
d2001 9
d2013 1
d2069 1
a2069 1
			*pte = template | ap | trunc_page(pa);
a2077 6
		if (va == phys_map_vaddr1 || va == phys_map_vaddr2) {
			flush_atc_entry(users, va, TRUE);
		} else {
			pmap_remove_range(pmap, va, va + PAGE_SIZE);
		}

d2090 5
a2116 1
				pvl->pv_flags = 0;
d2135 3
a2137 8
				pv_e = pool_get(&pvpool, PR_NOWAIT);
				if (pv_e == NULL) {
					if (flags & PMAP_CANFAIL) {
						PMAP_UNLOCK(pmap, spl);
						return (ENOMEM);
					} else
						panic("pmap_enter: "
						    "pvpool exhausted");
a2141 1
				pv_e->pv_flags = 0;
d2143 4
d2175 3
d2326 1
a2326 1
pmap_collect(pmap_t pmap)
d2330 1
d2349 2
a2350 1
	sdtp = pmap->pm_stab; /* addr of segment table */
d2364 1
a2364 1
		sdt_va = PDT_TABLE_GROUP_VA_SPACE * i;
d2739 1
d2759 3
d2768 1
a2768 1
		return (FALSE);
d2855 3
a2857 3
	if (!(prot & VM_PROT_READ))
		pmap_remove_all(pg);
	else if (!(prot & VM_PROT_WRITE))
d2860 8
a2926 1
	vaddr_t e;
d2939 2
a2940 2
	e = va + round_page(len);
	for (; va < e; va += PAGE_SIZE) {
@


1.1.1.3
log
@Synchronize with OpenBSD 3.4-beta
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2003/08/20 19:35:50 miod Exp $	*/
a230 1
void flush_atc_entry(long, vaddr_t, boolean_t);
d274 1
a274 1
			if ((p & PG_RO) == 0)
d296 2
d318 4
@


1.1.1.4
log
@Release Time. Synchronize with OpenBSD 3.4-current (base system).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2003/09/19 23:12:22 miod Exp $	*/
d224 3
d593 1
a593 2
	vaddr_t va;
	paddr_t pa;
d641 1
a641 1
		pa = ptoa(PG_PFNUM(*pte));
d644 1
a644 1
				cmmu_flush_remote_cache(cpu, pa, PAGE_SIZE);
a868 1
#ifdef DEBUG
d875 1
d880 1
a883 1
#endif
d945 1
a945 1
	ptable = pmap_table_build();
d954 3
d958 1
a958 1
			    ptable->phys_start + ptable->size,
a1005 1

d1205 2
a1206 1
		    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_INH);
d2601 1
a2601 1
	pt_entry_t *pte, npte, opte;
d2659 2
a2660 2
		opte = invalidate_pte(pte);
		npte = (opte | set) & mask;
d2668 2
a2669 2
		*pte = npte;
		if (npte != opte) {
@


1.1.1.5
log
@cvs is playing games with me.

@@@@@@ CONSIDER THE TREE LOCKED NOW @@@@@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2003/09/26 22:27:26 miod Exp $	*/
d113 1
a113 15

/*
 * Alignment checks for pages (must lie on page boundaries).
 */

#define PAGE_ALIGNED(ad)	(((vm_offset_t)(ad) & PAGE_MASK) == 0)
#define	CHECK_PAGE_ALIGN(ad,who) \
	if (!PAGE_ALIGNED(ad)) \
		printf("%s: addr  %x not page aligned.\n", who, ad)

#else	/* DEBUG */

#define	CHECK_PAGE_ALIGN(ad,who)

#endif	/* DEBUG */
d465 1
a465 1
	u_int32_t batctmp;
d483 2
a484 1
	batctmp = BATC_SO | BATC_V;
d486 1
a486 1
		batctmp |= BATC_WT;
d488 1
a488 1
		batctmp |= BATC_GLOBAL;
d490 1
a490 1
		batctmp |= BATC_INH;
d492 2
a493 1
		batctmp |= BATC_PROT;
d516 2
a517 2
			batctmp |= M88K_BTOBLK(virt) << BATC_VSHIFT;
			batctmp |= M88K_BTOBLK(page) << BATC_PSHIFT;
d522 2
a523 2
					    batctmp);
			batc_entry[batc_used] = batctmp;
d526 1
a526 1
				printf("(pmap_map: %x) BATC used=%d, data=%x\n", curproc, batc_used, batctmp);
d597 1
a597 1
	if ((mode & CACHE_MASK) != mode) {
d633 1
a633 1
		*pte = (invalidate_pte(pte) & ~CACHE_MASK) | mode;
d702 1
a702 1
	u_int32_t apr_data;
d712 2
d1006 6
a1011 1
	apr_data = (atop(kernel_pmap->pm_stpa) << PG_SHIFT) | CACHE_WT | APR_V;
d1014 1
a1014 1
		show_apr(apr_data);
d1020 1
a1020 1
		printf("invalidating tlb %x\n", apr_data);
d1039 1
a1039 1
			cmmu_remote_set_sapr(i, apr_data);
a1183 1
#ifdef DEBUG
d1188 1
d2426 1
a2426 1
	u_int32_t apr_data;
d2445 6
a2450 2
		apr_data = (atop(pmap->pm_stpa) << PG_SHIFT) |
		    CACHE_GLOBAL | APR_V;
d2459 2
a2460 2
		cmmu_pmap_activate(cpu, apr_data,
		    pmap->pm_ibatc, pmap->pm_dbatc);
d2464 1
a2464 1
		cmmu_set_uapr(apr_data);
@


1.1.1.6
log
@Import OpenBSD source tree again, with critical bug fixes
(OpenSSL, bc, dc, sensorsd, pf, ...)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2003/09/29 20:29:04 miod Exp $	*/
d182 1
a182 1
 * Two pages of scratch space per cpu.
d185 1
a185 1
vaddr_t phys_map_vaddr1, phys_map_vaddr2, phys_map_vaddr_end;
d490 1
a490 2
	/* Check for zero if we map the very end of the address space... */
	if (start > end && end != 0) {
a491 1
	}
d616 1
d618 1
d621 1
a621 1
#endif /* DEBUG */
d696 2
a697 2
 *    A pair of virtual pages per cpu are reserved for debugging and
 * IO purposes. They are arbitrarily mapped when needed. They are used,
d933 1
a933 2
	phys_map_vaddr2 = phys_map_vaddr1 + (max_cpus << PAGE_SHIFT);
	phys_map_vaddr_end = phys_map_vaddr2 + 2 * (max_cpus << PAGE_SHIFT);
d1039 1
a1039 2
			pte = pmap_pte(kernel_pmap,
			    phys_map_vaddr1 + (i << PAGE_SHIFT));
d1041 1
a1041 2
			pte = pmap_pte(kernel_pmap,
			    phys_map_vaddr2 + (i << PAGE_SHIFT));
d1067 9
d1127 1
a1127 1
	srcva = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
d1200 9
a1208 4
	/* memory for page tables should be CACHE DISABLED */
	pmap_cache_ctrl(kernel_pmap,
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_INH);

d1855 8
a1862 3
	/* memory for page tables should be CACHE DISABLED */
	pmap_cache_ctrl(kernel_pmap,
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_INH);
d2066 1
a2066 1
		if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
d2075 1
a2075 1
			if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
d2088 1
a2088 1
				if (va >= phys_map_vaddr1 && va < phys_map_vaddr_end) {
d2647 1
a2647 2
			printf("pmap_changebit: invalid pte %x pg %x %x\n",
			    *pte, pg, VM_PAGE_TO_PHYS(pg));
d2649 1
a2649 2
			panic("pmap_changebit: pte %x doesn't point to page %x %x\n",
			    *pte, pg, VM_PAGE_TO_PHYS(pg));
@


1.1.1.7
log
@Time to import OpenBSD once again. Expect breakage.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2003/12/19 22:30:18 miod Exp $	*/
d67 1
a67 1
 * VM externals
d69 2
a70 2
extern vaddr_t	avail_start, avail_end;
extern vaddr_t	virtual_avail, virtual_end;
d86 2
d108 1
a108 1
#define CD_USBIT	0x0100000	/* pmap_unsetbit */
d112 1
a112 1
int pmap_con_dbg = 0;
d117 3
a119 2
#define PAGE_ALIGNED(ad)	(((vaddr_t)(ad) & PAGE_MASK) == 0)
#define	CHECK_PAGE_ALIGN(ad, who) \
d121 1
a121 1
		printf("%s: addr %x not page aligned.\n", who, ad)
d125 1
a125 1
#define	CHECK_PAGE_ALIGN(ad, who)
a243 1
void pmap_remove_pte(pmap_t, vaddr_t, pt_entry_t *);
a250 1
boolean_t pmap_unsetbit(struct vm_page *, int);
d257 2
d300 1
a300 1
 *	'kernel'.
d306 1
a306 1
 *	kernel	TRUE if supervisor mode, FALSE if user mode
d314 1
a314 1
#ifdef DEBUG
d316 2
a317 1
		panic("flush_atc_entry: invalid ff1 users = %d", ff1(tusers));
d357 1
a357 1
	sdt = SDTENT(pmap, virt);
d359 1
a359 1
	 * Check whether page table exists.
d408 1
a408 1
		printf("(pmap_expand_kmap: %x) v %x\n", curproc, virt);
a414 1
#ifdef DEBUG
a416 1
#endif
d427 2
a428 3

	/* Reinitialize this kpdt area to zero */
	bzero((void *)kpdt_ent, PDT_SIZE);
d484 1
a484 1
	if (pmap_con_dbg & CD_MAP)
d539 1
a539 1
			if (pmap_con_dbg & CD_MAP) {
d541 2
a548 1
			}
d557 1
a557 1

d595 1
a595 1
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
d612 1
a612 1
		printf("(cache_ctrl) illegal mode %x\n", mode);
d615 1
a615 1
	if (pmap_con_dbg & CD_CACHE) {
d636 1
a636 1
		if (pmap_con_dbg & CD_CACHE) {
d714 1
d721 1
a721 1
	if (pmap_con_dbg & CD_BOOT) {
d737 1
a737 1
	 *  pmap_bootstrap(&kernelstart, ...);
d754 1
a754 1
	kmap = (sdt_entry_t *)(*phys_start);
d760 2
a761 2
		printf("kernel_pmap->pm_stab = 0x%x (pa 0x%x)\n",
		    kernel_pmap->pm_stab, kmap);
d781 1
d783 1
d791 1
a791 1

d795 1
a795 1

d799 1
a799 1

d819 2
a820 2
		printf("kpdt_phys = 0x%x\n", kpdt_phys);
		printf("kpdt_virt = 0x%x\n", kpdt_virt);
d822 1
a822 1
		    *virt_start, *phys_start);
d841 1
a841 1
	s_text = load_start;	/* paddr of text */
d854 1
a854 1
	    CACHE_GLOBAL);	/* shouldn't it be RO? XXX*/
d886 1
a886 1
			    *virt_start, *phys_start);
d895 4
a898 4
	 * Get ethernet buffer - need etherlen bytes physically contiguous.
	 * 1 to 1 mapped as well???. There is actually a bug in the macros
	 * used by the 1x7 ethernet driver. Remove this when that is fixed.
	 * XXX -nivas
d915 1
a915 1
				    *virt_start, *phys_start);
d961 1
a961 1
	for (; ptable->size != (vsize_t)(-1); ptable++){
d981 2
a982 2
	if ((p = pmap_pte(kernel_pmap, virt)) == PT_ENTRY_NULL) \
		pmap_expand_kmap(virt, VM_PROT_READ | VM_PROT_WRITE); \
d1017 1
a1017 1
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) | CACHE_WT | APR_V;
d1020 1
a1020 1
		show_apr(kernel_pmap->pm_apr);
d1024 5
d1042 1
a1042 1
			invalidate_pte(pte);
d1045 1
a1045 1
			invalidate_pte(pte);
d1047 1
a1047 1
			cmmu_remote_set_sapr(i, kernel_pmap->pm_apr);
d1080 1
a1080 1
	if (pmap_con_dbg & CD_INIT)
a1119 2
	CHECK_PAGE_ALIGN(pa, "pmap_zero_page");

d1126 1
a1126 1
	*srcpte = pa |
a1151 1
	paddr_t stpa;
d1165 1
a1165 1
	if (pmap_con_dbg & CD_CREAT) {
d1167 1
a1167 1
		    curproc, atop(s));
d1173 2
a1174 2
		panic("pmap_create: uvm_km_zalloc failure");

d1180 1
a1180 1
	    (paddr_t *)&stpa) == FALSE)
a1181 1
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) | CACHE_GLOBAL | APR_V;
d1184 1
a1184 1
	if (!PAGE_ALIGNED(stpa))
d1186 1
a1186 1
		    (int)stpa);
d1188 3
a1190 3
	if (pmap_con_dbg & CD_CREAT) {
		printf("(pmap_create: %x) pmap=0x%p, pm_stab=0x%x (pa 0x%x)\n",
		    curproc, pmap, pmap->pm_stab, stpa);
d1194 1
a1194 1
	/* memory for page tables should not be writeback or local */
d1196 1
a1196 1
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_WT | CACHE_GLOBAL);
d1202 1
a1202 1
	 * There is no need to clear segment table, since uvm_km_zalloc
d1236 1
d1243 1
a1243 1
 * all translation table space back to the system using uvm_km_free.
d1245 1
a1245 1
 * ranges represented by the table group sizes(PDT_VA_SPACE).
d1254 1
d1257 1
a1257 1
	if (pmap_con_dbg & CD_FREE)
d1261 11
d1273 2
a1274 2
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE) {
d1279 1
a1279 1
				    curproc, gdttbl);
d1281 1
a1281 1
			uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
d1283 1
a1283 1
	}
a1284 4
	/*
	 * Freeing both *actual* and *shadow* segment tables
	 */
	sdttbl = pmap->pm_stab;		/* addr of segment table */
d1288 1
a1288 1
		    curproc, sdttbl);
d1290 4
a1293 1
	uvm_km_free(kernel_map, (vaddr_t)sdttbl, round_page(2 * SDT_SIZE));
d1296 1
a1296 1
	if (pmap_con_dbg & CD_FREE)
d1347 1
a1347 1
 *	Add a reference to the specified pmap.
d1365 1
a1365 1
 * Routine:	PMAP_REMOVE_PTE (internal)
d1368 3
a1370 2
 *	Invalidate a given page table entry associated with the
 *	given virtual address.
d1374 2
a1375 2
 *	va		virtual address of page to remove
 *	pte		existing pte
d1381 1
d1389 7
a1395 1
 *  If the PTE is valid, the routine must invalidate the entry. The
d1403 1
a1403 1
pmap_remove_pte(pmap_t pmap, vaddr_t va, pt_entry_t *pte)
d1405 1
a1405 1
	pt_entry_t opte;
d1409 1
d1413 3
a1415 13
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove_pte: %x) pmap kernel va %x\n", curproc, va);
		else
			printf("(pmap_remove: %x) pmap %x va %x\n", curproc, pmap, va);
	}
#endif

	if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
		return;	 	/* no page mapping, nothing to do! */
	}

d1424 3
a1426 1
	 * Update statistics.
a1427 3
	pmap->pm_stats.resident_count--;
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
d1429 4
a1432 1
	pa = ptoa(PG_PFNUM(*pte));
d1434 8
a1441 3
	/*
	 * Invalidate the pte.
	 */
d1443 1
a1443 2
	opte = invalidate_pte(pte) & (PG_U | PG_M);
	flush_atc_entry(users, va, kflush);
d1445 3
a1447 1
	pg = PHYS_TO_VM_PAGE(pa);
d1449 9
a1457 3
	/* If this isn't a managed page, just return. */
	if (pg == NULL)
		return;
d1459 6
a1464 5
	/*
	 * Remove the mapping from the pvlist for
	 * this physical page.
	 */
	pvl = pg_to_pvh(pg);
d1467 2
a1468 2
	if (pvl->pv_pmap == PMAP_NULL)
		panic("pmap_remove_pte: null pv_list");
d1471 13
a1483 11
	prev = PV_ENTRY_NULL;
	for (cur = pvl; cur != PV_ENTRY_NULL; cur = cur->pv_next) {
		if (cur->pv_va == va && cur->pv_pmap == pmap)
			break;
		prev = cur;
	}
	if (cur == PV_ENTRY_NULL) {
		panic("pmap_remove_pte: mapping for va "
		    "0x%lx (pa 0x%lx) not in pv list at 0x%p",
		    va, pa, pvl);
	}
d1485 1
a1485 18
	if (prev == PV_ENTRY_NULL) {
		/*
		 * Hander is the pv_entry. Copy the next one
		 * to hander and free the next one (we can't
		 * free the hander)
		 */
		cur = cur->pv_next;
		if (cur != PV_ENTRY_NULL) {
			cur->pv_flags = pvl->pv_flags;
			*pvl = *cur;
			pool_put(&pvpool, cur);
		} else {
			pvl->pv_pmap = PMAP_NULL;
		}
	} else {
		prev->pv_next = cur->pv_next;
		pool_put(&pvpool, cur);
	}
d1487 11
a1497 3
	/* Update saved attributes for managed page */
	pvl->pv_flags |= opte;
}
d1499 4
a1502 33
/*
 * Routine:	PMAP_REMOVE_RANGE (internal)
 *
 * Function:
 *	Invalidate page table entries associated with the
 *	given virtual address range. The entries given are the first
 *	(inclusive) and last (exclusive) entries for the VM pages.
 *
 * Parameters:
 *	pmap		pointer to pmap structure
 *	s		virtual address of start of range to remove
 *	e		virtual address of end of range to remove
 *
 * External/Global:
 *	pv lists
 *
 * Calls:
 *	pmap_pte
 *	pmap_remove_pte
 *
 * Special Assumptions:
 *	The pmap must be locked.
 *
 *   This routine sequences through the pages defined by the given
 * range. For each page, the associated page table entry (PTE) is
 * invalidated via pmap_remove_pte().
 *
 * Empty segments are skipped for performance.
 */
void
pmap_remove_range(pmap_t pmap, vaddr_t s, vaddr_t e)
{
	vaddr_t va;
d1504 4
a1507 8
#ifdef DEBUG
	if (pmap_con_dbg & CD_RM) {
		if (pmap == kernel_pmap)
			printf("(pmap_remove: %x) pmap kernel s %x e %x\n", curproc, s, e);
		else
			printf("(pmap_remove: %x) pmap %x s %x e %x\n", curproc, pmap, s, e);
	}
#endif
d1509 6
a1514 5
	/*
	 * Loop through the range in vm_page_size increments.
	 */
	for (va = s; va < e; va += PAGE_SIZE) {
		sdt_entry_t *sdt;
d1516 5
a1520 7
		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
			continue;
d1523 1
a1523 2
		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
	}
d1556 5
d1612 3
a1624 5
#ifdef DEBUG
	if (pmap_con_dbg & CD_RMAL)
		printf("(pmap_remove_all: %x) va %x\n", curproc, pg, pg_to_pvh(pg)->pv_va);
#endif

d1645 14
a1658 1
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
d1672 1
a1672 1
		pmap_remove_pte(pmap, va, pte);
d1674 3
d1703 1
d1705 1
d1710 1
a1710 1
 * entry (PTE). If the PTE is invalid, or non-existent, nothing is done.
d1746 1
a1746 1
	 * Loop through the range in vm_page_size increments.
d1749 14
a1762 8
		sdt_entry_t *sdt;

		sdt = SDTENT(pmap, va);

		/* If no segment table, skip a whole segment */
		if (!SDT_VALID(sdt)) {
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1766 6
a1771 3
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
d1791 1
a1791 1
 *	New kernel virtual memory is allocated for a page table.
d1796 1
a1796 1
 *	that checks whether the map has been expanded enough. (We won't loop
d1815 1
a1815 1
 *	pmap != kernel_pmap
d1826 1
a1826 1
	int spl;
d1833 1
a1833 1
	if (pmap_con_dbg & CD_EXP)
d1844 1
a1844 1
	/* memory for page tables should not be writeback or local */
d1846 1
a1846 1
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_WT | CACHE_GLOBAL);
d1855 2
a1856 1
		simple_unlock(&pmap->pm_lock);
a1862 1
		splx(spl);
d1867 2
a1868 2
	 * its containing page 'table group', i.e. the group of
	 * page tables that fit eithin a single VM page.
d1873 1
a1873 1
	v &= ~((1 << (PDT_BITS + PG_BITS)) - 1);
d1875 1
a1875 1
	sdt = SDTENT(pmap, v);
d1881 7
a1887 3
	*((sdt_entry_t *)sdt) = pdt_paddr | SG_RW | SG_V;
	*((sdt_entry_t *)(sdt + SDT_ENTRIES)) = pdt_vaddr | SG_RW | SG_V;

d1918 2
a1919 1
 *	pmap_remove_pte
d1926 1
a1926 1
 *	If the page table entry (PTE) already maps the given physical page,
d1942 1
a1942 1
 *	if necessary pmap_expand(pmap, v)
d1950 1
a1950 1
 *		pmap_remove_pte
d1961 1
a1961 1
	pt_entry_t *pte, template;
d1969 2
a1970 2
	CHECK_PAGE_ALIGN(va, "pmap_entry - va");
	CHECK_PAGE_ALIGN(pa, "pmap_entry - pa");
d1973 1
a1973 1
	if (pmap_con_dbg & CD_ENT) {
d1981 1
a1981 1
	template = m88k_protection(pmap, prot);
a1985 6
	if (pmap == kernel_pmap) {
		kflush = TRUE;
	} else {
		kflush = FALSE;
	}

d1991 1
a1991 1
			pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
d1996 1
a1996 1
			simple_unlock(&pmap->pm_lock);
d1998 1
a1998 1
			simple_lock(&pmap->pm_lock);
d2002 2
a2003 1
	 * Special case if the physical page is already mapped at this address.
a2005 4
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
#endif
d2007 6
d2022 6
a2027 2
		pg = NULL;
	} else { /* if (pa == old_pa) */
d2030 17
d2053 1
a2053 1
			pmap_remove_pte(pmap, va, pte);
d2058 1
a2058 1
		if (pmap_con_dbg & CD_ENT) {
d2061 2
a2062 2
				    phys_map_vaddr1, phys_map_vaddr2, va, old_pa,
				    pg != NULL ? 1 : 0);
d2064 1
a2064 1
				    pte, PG_PFNUM(*pte), PDT_VALID(pte));
d2071 1
a2071 1
			if (pmap_con_dbg & CD_ENT) {
d2078 1
a2078 1
			 *	Enter the mapping in the PV list for this
a2131 1
	} /* if (pa == old_pa) ... else */
d2133 6
a2138 3
	template |= PG_V;
	if (wired)
		template |= PG_W;
d2140 4
a2143 4
	if ((unsigned long)pa >= MAXPHYSMEM)
		template |= CACHE_INH;
	else
		template |= CACHE_GLOBAL;
d2145 1
a2145 4
	if (flags & VM_PROT_WRITE)
		template |= PG_U | PG_M;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;
d2147 1
a2147 20
	/*
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
	 */
	template |= invalidate_pte(pte) & (PG_U | PG_M);
	*pte = template | pa;
	flush_atc_entry(users, va, kflush);
#ifdef DEBUG
	if (pmap_con_dbg & CD_ENT)
		printf("(pmap_enter) set pte to %x\n", *pte);
#endif

	/*
	 * Cache attribute flags
	 */
	if (pg != NULL) {
		pvl = pg_to_pvh(pg);
		pvl->pv_flags |= (template & (PG_U | PG_M));
	}
d2247 8
a2254 7
	pte = pmap_pte(pmap, va);
	if (pte != PT_ENTRY_NULL && PDT_VALID(pte)) {
		rv = TRUE;
		if (pap != NULL) {
			pa = ptoa(PG_PFNUM(*pte));
			pa |= (va & PAGE_MASK); /* offset within page */
			*pap = pa;
d2275 1
a2277 1
 *	uvm_km_free
d2288 1
a2288 1
 * invalidated. Finally, uvm_km_free is called to return the page to the
d2293 4
d2302 1
d2304 1
d2312 1
d2315 1
a2315 1
	if (pmap_con_dbg & CD_COL)
d2323 10
d2334 3
a2336 2
	for (sdt_va = VM_MIN_ADDRESS; sdt_va < VM_MAX_ADDRESS;
	    sdt_va += PDT_VA_SPACE, sdtp++) {
d2338 1
d2342 1
a2342 1
		gdttblend = gdttbl + PDT_ENTRIES;
d2356 4
d2361 1
a2361 1
		pmap_remove_range(pmap, sdt_va, sdt_va + PDT_VA_SPACE);
d2366 4
a2369 2
		*((sdt_entry_t *) sdtp) = 0;
		*((sdt_entry_t *)(sdtp + SDT_ENTRIES)) = 0;
d2372 3
a2374 2
		 * we have to unlock before freeing the table, since
		 * uvm_km_free will invoke another pmap routine
d2376 3
a2378 3
		simple_unlock(&pmap->pm_lock);
		uvm_km_free(kernel_map, (vaddr_t)gdttbl, PAGE_SIZE);
		simple_lock(&pmap->pm_lock);
d2384 1
a2384 1
	if (pmap_con_dbg & CD_COL)
d2393 6
a2398 1
 * 	Binds the pmap associated to the process to the current processor.
d2404 2
a2405 2
 *	If the specified pmap is not kernel_pmap, this routine stores its
 *	apr template into UAPR (user area pointer register) in the
d2410 1
d2415 1
d2418 1
d2424 1
a2424 1
	if (pmap_con_dbg & CD_ACTIVATE)
d2432 1
d2434 2
d2440 1
a2440 1
		 * then flush the *USER* TLB. IF THE KERNEL WILL EVER CARE
d2444 1
a2444 1
		cmmu_pmap_activate(cpu, pmap->pm_apr,
d2449 1
a2449 1
		cmmu_set_uapr(pmap->pm_apr);
d2457 1
d2466 2
a2467 1
 *	Unbinds the pmap associated to the process from the current processor.
d2470 4
a2473 1
 *	p		pointer to proc structure
d2480 1
a2480 1

d2509 1
a2509 1
 *	no locking required
a2524 3
	CHECK_PAGE_ALIGN(src, "pmap_copy_page - src");
	CHECK_PAGE_ALIGN(dst, "pmap_copy_page - dst");

d2539 1
a2539 1
	*srcpte = template | src;
d2545 1
a2545 1
	*dstpte = template | dst;
d2600 1
a2600 1
		if (pmap_con_dbg & CD_CBIT)
d2611 1
a2621 1
		va = pvep->pv_va;
d2624 1
d2628 5
a2632 4
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
d2634 2
a2635 2
			panic("pmap_changebit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
d2641 1
a2641 1
		opte = *pte;
d2650 1
a2651 2
			invalidate_pte(pte);
			*pte = npte;
d2654 1
a2654 1
next:
d2661 16
d2697 1
a2697 1
 * given page. For each pmap/va pair, the page descriptor table entry is
d2711 1
a2712 1
	pvl = pg_to_pvh(pg);
d2715 1
a2715 1
		/* we've already cached this flag for this page,
d2718 1
a2718 1
		if (pmap_con_dbg & CD_TBIT)
d2728 1
a2728 1
		if (pmap_con_dbg & CD_TBIT)
d2737 2
a2738 1
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
d2744 3
a2746 2
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;
d2748 1
a2748 8

#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_testbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pvep->pv_pmap, pvep->pv_pmap == kernel_pmap ? 1 : 0, pg, VM_PAGE_TO_PHYS(pg));
#endif

		if ((*pte & bit) != 0) {
d2753 1
a2753 1
				printf("(pmap_testbit: %x) true on page pte@@0x%p\n", curproc, pte);
a2757 1
next:
d2759 1
d2767 1
a2767 1
 * Routine:	PMAP_UNSETBIT
d2770 2
a2771 6
 *	Clears a pte bit and returns its previous state, for the
 *	specified physical page.
 *	This is an optimized version of:
 *		rv = pmap_testbit(pg, bit);
 *		pmap_changebit(pg, 0, ~bit);
 *		return rv;
d2774 1
a2774 1
pmap_unsetbit(struct vm_page *pg, int bit)
d2776 1
a2776 79
	boolean_t rv = FALSE;
	pv_entry_t pvl, pvep;
	pt_entry_t *pte, opte;
	pmap_t pmap;
	int spl;
	vaddr_t va;
	u_int users;
	boolean_t kflush;

	SPLVM(spl);

unsetbit_Retry:
	pvl = pg_to_pvh(pg);

	/*
	 * Clear saved attributes
	 */
	pvl->pv_flags &= ~bit;

	if (pvl->pv_pmap == PMAP_NULL) {
#ifdef DEBUG
		if (pmap_con_dbg & CD_USBIT)
			printf("(pmap_unsetbit: %x) vm page 0x%x not mapped\n",
			    curproc, pg);
#endif
		SPLX(spl);
		return (FALSE);
	}

	/* for each listed pmap, update the specified bit */
	for (pvep = pvl; pvep != PV_ENTRY_NULL; pvep = pvep->pv_next) {
		pmap = pvep->pv_pmap;
		if (!simple_lock_try(&pmap->pm_lock)) {
			goto unsetbit_Retry;
		}
		users = pmap->pm_cpus;
		if (pmap == kernel_pmap) {
			kflush = TRUE;
		} else {
			kflush = FALSE;
		}

		va = pvep->pv_va;
		pte = pmap_pte(pmap, va);

		/*
		 * Check for existing and valid pte
		 */
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			goto next;	 /* no page mapping */
		}
#ifdef DIAGNOSTIC
		if (ptoa(PG_PFNUM(*pte)) != VM_PAGE_TO_PHYS(pg))
			panic("pmap_unsetbit: pte %x in pmap %p %d doesn't point to page %p %lx",
			    *pte, pmap, kflush, pg, VM_PAGE_TO_PHYS(pg));
#endif

		/*
		 * Update bits
		 */
		opte = *pte;
		if (opte & bit) {
			/*
			 * Flush TLB of which cpus using pmap.
			 *
			 * Invalidate pte temporarily to avoid the specified
			 * bit being written back by any other cpu.
			 */
			invalidate_pte(pte);
			*pte = opte ^ bit;
			flush_atc_entry(users, va, kflush);
		} else
			rv = TRUE;
next:
		simple_unlock(&pmap->pm_lock);
	}
	SPLX(spl);

	return (rv);
d2780 1
a2780 1
 * Routine:	PMAP_IS_MODIFIED
d2783 1
a2783 2
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
d2786 1
a2786 1
pmap_is_modified(struct vm_page *pg)
d2788 5
a2792 1
	return pmap_testbit(pg, PG_M);
d2845 1
a2845 1
	if (pmap_con_dbg & CD_ENT) {
d2858 3
a2860 2
	while ((pte = pmap_pte(kernel_pmap, va)) == PT_ENTRY_NULL)
		pmap_expand_kmap(va, VM_PROT_READ | VM_PROT_WRITE);
d2873 1
a2873 1
	*pte = template | pa;
d2887 1
a2887 1
	if (pmap_con_dbg & CD_RM)
a2903 1
		/* If no segment table, skip a whole segment */
d2905 5
a2909 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
a2912 5
		pte = pmap_pte(kernel_pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}

d2919 1
@


1.1.1.8
log
@Import OpenBSD again, for various reasons.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2003/12/30 06:45:55 miod Exp $	*/
d346 1
a346 1
#ifdef DEBUG
d624 5
a628 1
	kflush = pmap == kernel_pmap;
d897 1
a897 1
		etherlen = ETHERPAGES * PAGE_SIZE;
d984 1
a984 1
	invalidate_pte(vmpte);
d1012 1
a1012 2
	kernel_pmap->pm_apr =
	    (atop(kmap) << PG_SHIFT) | CACHE_GLOBAL | CACHE_WT | APR_V;
d1105 1
a1105 1
	vaddr_t va;
d1107 2
a1108 2
	int cpu = cpu_number();
	pt_entry_t *pte;
d1112 3
a1114 2
	va = (vaddr_t)(phys_map_vaddr1 + (cpu << PAGE_SHIFT));
	pte = pmap_pte(kernel_pmap, va);
d1117 4
a1120 5

	cmmu_flush_tlb(TRUE, va, PAGE_SIZE);
	*pte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | pa;

d1123 3
a1125 1
	bzero((void *)va, PAGE_SIZE);
d1139 1
a1139 1
pmap_t
d1175 1
a1175 2
	pmap->pm_apr =
	    (atop(stpa) << PG_SHIFT) | CACHE_WT | CACHE_GLOBAL | APR_V;
d1310 1
a1310 1
#ifdef DEBUG
d1400 5
a1404 1
	kflush = pmap == kernel_pmap;
d1506 1
a1506 1
	vaddr_t va, seva;
d1520 1
a1520 1
	for (va = s; va < e;) {
a1523 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d1527 2
a1528 1
			va = seva;
d1532 1
a1532 4
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE)
			pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
d1653 12
a1664 3
			pvl = pvl->pv_next;	/* no page mapping */
		} else {
			pmap_remove_pte(pmap, va, pte);
d1666 1
a1666 5
			/*
			 * Do not free any page tables,
			 * leave that for when VM calls pmap_collect().
			 */
		}
d1668 5
d1707 1
a1707 1
	vaddr_t va, seva;
d1711 5
d1717 1
a1717 1
	if (s >= e)
a1720 5
	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, s, e);
		return;
	}

d1726 5
a1730 1
	kflush = pmap == kernel_pmap;
d1737 1
a1737 1
	for (va = s; va < e;) {
a1740 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d1744 2
a1745 1
			va = seva;
d1749 4
a1752 7
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE) {
			pte = pmap_pte(pmap, va);
			if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
				continue;	 /* no page mapping */
			}
d1754 8
a1761 11
			if ((*pte & PG_PROT) ^ ap) {
				/*
				 * Invalidate pte temporarily to avoid the
				 * modified bit and/or the reference bit being
				 * written back by any other cpu.
				 */
				*pte = (invalidate_pte(pte) ^ PG_PROT);
				flush_atc_entry(users, va, kflush);
			}
			pte++;
		}
d1936 1
a1936 1
	pt_entry_t *pte, opte, template;
d1960 6
a1965 1
	kflush = pmap == kernel_pmap;
d1985 1
a1985 2
	opte = *pte;
	old_pa = ptoa(PG_PFNUM(opte));
d1988 1
a1988 1
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, opte);
d1991 5
a1995 2
		/* May be changing its wired attributes or protection */
		if (wired && !(pmap_pte_w(&opte)))
d1997 1
a1997 1
		else if (!wired && pmap_pte_w(&opte))
d1999 2
d2002 9
a2010 3
		if (old_pa != NULL) {
			if (va < phys_map_vaddr1 || va >= phys_map_vaddr_end)
				pmap_remove_pte(pmap, va, pte);
d2021 1
a2021 1
				    pte, PG_PFNUM(opte), PDT_VALID(&opte));
d2052 1
a2052 1
				 * Check that this mapping is not already there
d2054 8
a2061 4
				for (pv_e = pvl; pv_e; pv_e = pv_e->pv_next)
					if (pv_e->pv_pmap == pmap &&
					    pv_e->pv_va == va)
						panic("pmap_enter: already in pv_list");
a2080 8

			/*
			 * Cache attribute flags
			 */
			if (flags & VM_PROT_WRITE)
				pvl->pv_flags |= PG_U | PG_M;
			else if (flags & VM_PROT_ALL)
				pvl->pv_flags |= PG_U;
d2100 5
a2104 1
	template |= (opte & (PG_U | PG_M)) | pa;
d2106 3
a2108 1
	 * No need to flush anything if it's just a wiring change...
d2110 3
a2112 12
	if ((template ^ opte) == PG_W) {
		*pte = template;
	} else {
		/*
		 * Invalidate pte temporarily to avoid being written
		 * back the modified bit and/or the reference bit by
		 * any other cpu.
		 */
		invalidate_pte(pte);
		*pte = template;
		flush_atc_entry(users, va, kflush);
	}
d2115 1
a2115 1
		printf("(pmap_enter) set pte to %x\n", template);
d2118 8
d2154 1
a2154 3
	pte = pmap_pte(pmap, v);
#ifdef DEBUG
	if (pte == PT_ENTRY_NULL)
a2155 1
#endif
d2158 1
a2158 1
		/* unwire mapping */
d2203 1
a2203 1
#ifdef DEBUG
d2456 1
a2456 1
	pt_entry_t *dstpte, *srcpte;
d2462 6
d2470 1
a2474 1

d2476 1
a2476 2
	*srcpte = m88k_protection(kernel_pmap, VM_PROT_READ) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | src;
d2478 3
d2482 1
a2482 3
	*dstpte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_WT | CACHE_GLOBAL | PG_V | dst;

d2486 3
d2552 5
a2556 1
		kflush = pmap == kernel_pmap;
d2741 5
a2745 1
		kflush = pmap == kernel_pmap;
d2824 1
a2824 1
	if ((prot & VM_PROT_READ) == VM_PROT_NONE)
d2826 2
a2827 1
	else if ((prot & VM_PROT_WRITE) == VM_PROT_NONE)
d2887 1
a2887 1
	vaddr_t e, seva;
d2900 2
a2901 1
	for (e = va + len; va < e;) {
a2905 1
		seva = (va & SDT_MASK) + (1 << SDT_SHIFT);
d2909 2
a2910 1
			va = seva;
d2914 4
a2917 7
		if (seva > e)
			seva = e;
		for (; va < seva; va += PAGE_SIZE) {
			pte = pmap_pte(kernel_pmap, va);
			if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
				continue;	 /* no page mapping */
			}
d2919 5
a2923 5
			/*
			 * Update the counts
			 */
			kernel_pmap->pm_stats.resident_count--;
			kernel_pmap->pm_stats.wired_count--;
d2925 2
a2926 3
			invalidate_pte(pte);
			flush_atc_entry(users, va, TRUE);
		}
@


1.1.1.9
log
@Import OpenBSD as of today again (seems pretty stable, I hope)

Prominent changes: more bgpd, tcpmd5; tcpdump/isakmpd fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2004/01/15 23:36:08 miod Exp $	*/
a66 5
 * Define this to allow write-back caching. Use at your own risk.
 */
#undef	PMAP_USE_WRITEBACK

/*
d182 1
a182 1
vaddr_t phys_map_vaddr, phys_map_vaddr_end;
d319 1
a319 1
			cmmu_flush_tlb(cpu, kernel, va, PAGE_SIZE);
d488 1
a488 1
#ifdef DEBUG
d648 1
a648 1
				cmmu_flush_cache(cpu, pa, PAGE_SIZE);
d674 2
a675 1
 *	phys_map_vaddr	VA of page mapped arbitrarily for debug/IO
d868 16
d891 1
a891 1
	if (brdtyp == BRD_187 || brdtyp == BRD_8120 || brdtyp == BRD_197) {
d920 10
a929 1
	 * Map two pages per cpu for copying/zeroing.
d932 2
a933 4
	phys_map_vaddr = *virt_start;
	phys_map_vaddr_end = *virt_start + 2 * (max_cpus << PAGE_SHIFT);
	*phys_start += 2 * (max_cpus << PAGE_SHIFT);
	*virt_start += 2 * (max_cpus << PAGE_SHIFT);
d1008 2
a1009 2
	kernel_pmap->pm_apr = (atop(kmap) << PG_SHIFT) |
	    CACHE_GLOBAL | CACHE_WT | APR_V;
d1015 2
a1016 1
	/* Invalidate entire kernel TLB and get ready for address translation */
d1019 14
a1032 2
			cmmu_flush_tlb(i, TRUE, VM_MIN_KERNEL_ADDRESS,
			    VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS);
d1034 1
a1034 1
			cmmu_set_sapr(i, kernel_pmap->pm_apr);
d1086 1
a1086 1
 *	phys_map_vaddr
d1109 1
a1109 1
	va = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
d1114 1
d1116 1
a1116 1
	    CACHE_GLOBAL | PG_V | pa;
d1118 1
a1118 5
	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_flush_tlb(cpu, TRUE, va, PAGE_SIZE);
a1119 5
	/*
	 * The page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the pa flushed after the filling.
	 */
a1120 5
#ifdef	PMAP_USE_WRITEBACK
	cmmu_flush_data_cache(cpu, pa, PAGE_SIZE);
#endif

	SPLX(spl);
d1170 2
a1171 5
	pmap->pm_apr = (atop(stpa) << PG_SHIFT) |
	    CACHE_GLOBAL | CACHE_WT | APR_V;
#ifdef	PMAP_USE_WRITEBACK
	pmap->pm_apr &= ~CACHE_WT;	/* enable writeback */
#endif
a1183 1
#ifdef	PMAP_USE_WRITEBACK
d1186 1
a1186 2
	    (vaddr_t)segdt, (vaddr_t)segdt + s, CACHE_GLOBAL | CACHE_WT);
#endif
d1498 1
a1498 1
	vaddr_t va;
d1512 1
a1512 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1516 1
d1520 1
a1520 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1524 4
a1527 1
		pmap_remove_pte(pmap, va, pmap_pte(pmap, va));
d1561 1
a1561 1
#ifdef DEBUG
d1648 8
a1655 2
			pvl = pvl->pv_next;
			goto next;	/* no page mapping */
a1656 11
		if (pmap_pte_w(pte)) {
#ifdef DEBUG
			if (pmap_con_dbg & CD_RMAL)
				printf("pmap_remove_all: wired mapping for %lx not removed\n",
				    pg);
#endif
			pvl = pvl->pv_next;
			goto next;
		}

		pmap_remove_pte(pmap, va, pte);
a1657 5
		/*
		 * Do not free any page tables,
		 * leaves that for when VM calls pmap_collect().
		 */
next:
d1692 1
a1692 1
	vaddr_t va;
d1696 1
a1696 1
#ifdef DEBUG
d1701 1
a1701 1
	if ((prot & VM_PROT_READ) == 0) {
d1718 1
a1718 1
	for (va = s; va < e; va += PAGE_SIZE) {
d1722 1
d1726 1
a1726 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d1730 18
a1747 3
		pte = pmap_pte(pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
a1748 9

		/*
		 * Invalidate pte temporarily to avoid the
		 * modified bit and/or the reference bit being
		 * written back by any other cpu.
		 */
		*pte = (invalidate_pte(pte) & ~PG_PROT) | ap;
		flush_atc_entry(users, va, kflush);
		pte++;
a1810 1
#ifdef	PMAP_USE_WRITEBACK
d1813 1
a1813 2
	    pdt_vaddr, pdt_vaddr + PAGE_SIZE, CACHE_GLOBAL | CACHE_WT);
#endif
d1923 1
a1923 1
	pt_entry_t *pte, template;
a1940 6

	/* copying/zeroing pages are magic */
	if (pmap == kernel_pmap &&
	    va >= phys_map_vaddr && va < phys_map_vaddr_end) {
		return 0;
	}
d1967 2
a1968 1
	old_pa = ptoa(PG_PFNUM(*pte));
d1971 1
a1971 1
		printf("(pmap_enter) old_pa %x pte %x\n", old_pa, *pte);
d1975 1
a1975 1
		if (wired && !(pmap_pte_w(pte)))
d1977 1
a1977 1
		else if (!wired && pmap_pte_w(pte))
a1978 2

		pvl = NULL;
d1980 4
a1983 2
		/* Remove old mapping from the PV list if necessary. */
		pmap_remove_pte(pmap, va, pte);
d1986 12
d1999 7
d2049 8
d2076 1
a2076 5
	if (flags & VM_PROT_WRITE)
		template |= PG_U | PG_M;
	else if (flags & VM_PROT_ALL)
		template |= PG_U;

d2078 1
a2078 3
	 * Invalidate pte temporarily to avoid being written
	 * back the modified bit and/or the reference bit by
	 * any other cpu.
d2080 12
a2091 3
	template |= invalidate_pte(pte) & (PG_U | PG_M);
	*pte = template | pa;
	flush_atc_entry(users, va, kflush);
d2094 1
a2094 1
		printf("(pmap_enter) set pte to %x\n", *pte);
a2096 6
	/*
	 * Cache attribute flags
	 */
	if (pvl != NULL)
		pvl->pv_flags |= (template & (PG_U | PG_M));

d2125 3
a2127 1
	if ((pte = pmap_pte(pmap, v)) == PT_ENTRY_NULL)
d2129 1
d2132 1
a2132 1
		/* unwired mapping */
d2177 1
a2177 1
#ifdef DIAGNOSTIC
d2327 2
a2328 2
 *	Then it flushes the TLBs mapping user virtual space, in the CMMUs
 *	connected to the specified CPU.
d2363 1
a2363 2
		cmmu_flush_tlb(cpu, FALSE, VM_MIN_ADDRESS,
		    VM_MAX_ADDRESS - VM_MIN_ADDRESS);
d2410 2
a2411 1
 *	phys_map_vaddr
d2436 3
a2438 2
	dstva = (vaddr_t)(phys_map_vaddr + 2 * (cpu << PAGE_SHIFT));
	srcva = (vaddr_t)(phys_map_vaddr + PAGE_SIZE + 2 * (cpu << PAGE_SHIFT));
a2439 1
	srcpte = pmap_pte(kernel_pmap, srcva);
d2443 1
a2443 2
	*dstpte = m88k_protection(kernel_pmap, VM_PROT_READ | VM_PROT_WRITE) |
	    CACHE_GLOBAL | PG_V | dst;
d2445 1
a2445 1
	    CACHE_GLOBAL | PG_V | src;
d2447 3
a2449 5
	/*
	 * We don't need the flush_atc_entry() dance, as these pages are
	 * bound to only one cpu.
	 */
	cmmu_flush_tlb(cpu, TRUE, dstva, 2 * PAGE_SIZE);
d2451 1
a2451 13
	/*
	 * The source page is likely to be a non-kernel mapping, and as
	 * such write back. Also, we might have split U/S caches!
	 * So be sure to have the source pa flushed before the copy is
	 * attempted, and the destination pa flushed afterwards.
	 */
#ifdef	PMAP_USE_WRITEBACK
	cmmu_flush_data_cache(cpu, src, PAGE_SIZE);
#endif
	bcopy((const void *)srcva, (void *)dstva, PAGE_SIZE);
#ifdef	PMAP_USE_WRITEBACK
	cmmu_flush_data_cache(cpu, dst, PAGE_SIZE);
#endif
d2453 1
a2453 1
	SPLX(spl);
d2600 2
a2601 2
			printf("(pmap_testbit: %x) already cached a %x flag for this page\n",
			    curproc, bit);
d2843 1
a2843 1
	vaddr_t e;
d2856 1
a2856 2
	e = va + len;
	for (; va < e; va += PAGE_SIZE) {
d2861 1
d2865 1
a2865 2
			va &= SDT_MASK;
			va += (1 << SDT_SHIFT) - PAGE_SIZE;
d2869 7
a2875 4
		pte = pmap_pte(kernel_pmap, va);
		if (pte == PT_ENTRY_NULL || !PDT_VALID(pte)) {
			continue;	 /* no page mapping */
		}
d2877 5
a2881 5
		/*
		 * Update the counts
		 */
		kernel_pmap->pm_stats.resident_count--;
		kernel_pmap->pm_stats.wired_count--;
d2883 3
a2885 2
		invalidate_pte(pte);
		flush_atc_entry(users, va, TRUE);
@


1.1.1.10
log
@large-scale import of OpenBSD 3.5-current source base including many fixes
note: from now, we will not be binary compatible with OpenBSD apps any
longer (due to syscall numbering differences); both an OpenBSD compat and
a conversion tool for old MirOS #7 apps will be delivered later.

The src/ tree is locked from now.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2004/05/07 15:30:04 miod Exp $	*/
d67 5
d74 1
a74 1
extern vaddr_t	avail_start;
a75 1
extern vaddr_t	last_addr;
d97 1
d149 35
d669 4
d690 3
a692 1
 * address space.
d705 2
a706 1
pmap_bootstrap(vaddr_t load_start)
d710 1
a710 1
	vaddr_t vaddr, virt;
a712 1
	unsigned int kernel_pmap_size, pdt_size;
d717 8
d735 5
a739 5
	 * kernelstart being the first symbol in the load image.
	 * The kernel is linked such that &kernelstart == 0x10000 (size of
	 * BUG reserved memory area).
	 * The expression (&kernelstart - load_start) will end up as
	 *	0, making virtual_avail == avail_start, giving a 1-to-1 map)
d742 2
a743 2
	avail_start = round_page(avail_start);
	virtual_avail = avail_start +
d751 10
a760 3
	kmap = (sdt_entry_t *)(avail_start);
	kernel_pmap->pm_stab = (sdt_entry_t *)virtual_avail;
	kmapva = virtual_avail;
d777 3
a779 1
	printf("kernel segment table size = 0x%x\n", kernel_pmap_size);
d784 2
a785 2
	avail_start += kernel_pmap_size;
	virtual_avail += kernel_pmap_size;
d788 2
a789 2
	avail_start = round_page(avail_start);
	virtual_avail = round_page(virtual_avail);
d792 2
a793 2
	kpdt_phys = avail_start;
	kpdt_virt = (kpdt_entry_t)virtual_avail;
d795 2
a796 6
	/* Compute how much space we need for the kernel page table */
	pdt_size = atop(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS)
	    * sizeof(pt_entry_t);
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		pdt_size += atop(ptable->size) * sizeof(pt_entry_t);
	pdt_size = round_page(pdt_size);
d798 2
a799 2
	avail_start += pdt_size;
	virtual_avail += pdt_size;
d804 7
a810 4
	printf("--------------------------------------\n");
	printf("        kernel page start = 0x%x\n", kpdt_phys);
	printf("   kernel page table size = 0x%x\n", pdt_size);
	printf("          kernel page end = 0x%x\n", avail_start);
d812 7
a818 1
	printf("kpdt_virt = 0x%x\n", kpdt_virt);
d861 6
a866 1
		while (vaddr < (virtual_avail - kernel_pmap_size))
d869 1
a869 1
	vaddr = pmap_map(vaddr, (paddr_t)kmap, avail_start,
d880 1
a880 1
		avail_start = vaddr;
d884 1
a884 1
		vaddr = pmap_map(vaddr, avail_start, avail_start + etherlen,
d887 2
a888 2
		virtual_avail += etherlen;
		avail_start += etherlen;
d890 9
a898 3
		if (vaddr != virtual_avail) {
			virtual_avail = vaddr;
			avail_start = round_page(avail_start);
d904 2
a905 2
	virtual_avail = round_page(virtual_avail);
	virtual_end = VM_MAX_KERNEL_ADDRESS;
d911 4
a914 4
	phys_map_vaddr = virtual_avail;
	phys_map_vaddr_end = virtual_avail + 2 * (max_cpus << PAGE_SHIFT);
	avail_start += 2 * (max_cpus << PAGE_SHIFT);
	virtual_avail += 2 * (max_cpus << PAGE_SHIFT);
d926 9
a934 2
	for (ptable = pmap_table_build(); ptable->size != (vsize_t)-1; ptable++)
		if (ptable->size != 0) {
d939 1
d958 1
a958 1
	virt = virtual_avail;
d965 1
a965 1
	virtual_avail = virt;
d976 5
d991 5
a995 1

d1004 3
a1006 1
			printf("cpu%d: running virtual\n", i);
d1010 6
d1097 1
d1099 1
d1152 4
a1155 1
	    CACHE_GLOBAL | APR_V;
d1168 1
d1172 1
d1798 1
d1802 1
d2038 1
a2038 1
	if ((unsigned long)pa >= last_addr)
d2404 1
a2404 1
	srcva = dstva + PAGE_SIZE;
d2427 1
d2429 1
d2431 1
d2433 1
d2810 1
a2810 1
	if ((unsigned long)pa >= last_addr)
@


